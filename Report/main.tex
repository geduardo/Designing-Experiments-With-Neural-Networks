%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% FRAME-FILE
%%
%% Author        : Stefan M. Moser,
%%
%% Written       : January 1998, based on a file by Michael Semling
%%
%% Modifications : ongoing, used for semester project 1, semester
%%                 project 2, master thesis, PPS, SADA @ ISI
%%
%% Last Modify.   : 21 May 2004, 24 Oct 2006 (Michèle Wigger)
%%
%% Version       : 3.4
%%
%% Copyright     : Free for distribution if 
%%                     - you keep this header (you may add to it!)
%%                     - you do not charge any fee
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% HELP:
%%
%% Compilation of file:
%%
%%  latex frame; bibtex frame; latex frame; latex frame
%%
%%  dvips -Pcmz -Pamz -Psuper -o frame.ps frame.dvi
%% 
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% HELP ONLINE:
%%
%% http://computing.ee.ethz.ch/.soft/latex/tetex/     (tardis)
%%
%% file:/usr/pack/tetex-1.0.7-mo/texmf/doc/index.html (ISI)
%%
%% CTAN for any package: http://www.ucc.ie/cgi-bin/ctan
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Styles: article, book, report
\documentclass[11pt,a4paper,twoside]{report}
\pdfoutput=1
%
%% Draft:
%
%\documentclass[draft,11pt,a4paper,twoside,dvips]{report}
                                    % On border a black ruler shows
                                    % lines with problems
%\usepackage[notref, notcite]{showkeys}
                                    % All labels are printed on the
                                    % side 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PACKAGES                         DESCRIPTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[ngerman,USenglish]{babel} % Different languages:
                                    % Swap between the languages
                                    % with \selectlanguage{sprache} 
%\usepackage[latin1]{inputenc}      % acceptance of german Umlaute


%\usepackage{bibtex/macros}  %for IEEE transaction submission it is not allowed to have a separate style-file :-( :-( :-(
\usepackage{blindtext}
\usepackage{etex}				%mehr register etc.
\usepackage{algorithmicx}
\usepackage{times}
%\usepackage{psfrag}
\usepackage{graphicx}
\usepackage{graphics}
%\usepackage{pstricks,pst-node,pst-tree}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{color}
\usepackage{fancyhdr}
%\PassOptionsToPackage{hyphens}{url}

%Raban: Added Joe's style
\usepackage[bookmarks,colorlinks,breaklinks]{hyperref}  % PDF hyperlinks, with coloured links
\definecolor{dullmagenta}{rgb}{0.4,0,0.4}   % #660066
\definecolor{darkblue}{rgb}{0,0,0.4}
\hypersetup{linkcolor=red,citecolor=blue,filecolor=dullmagenta,urlcolor=darkblue} % coloured links



%\usepackage[colorlinks,urlcolor=blue] {hyperref}    %change here colorlinks=false to remove the colored links (for print-version)
\usepackage{url}
%\usepackage[colorlinks, hyperindex,pagebackref] {hyperref}
%\usepackage{backref}
\urlstyle{tt}


%zusaetzlich nicht standard
%\usepackage{amsfonts}		%mathematisches Menge R, N
%\usepackage{mathrsfs}
%\usepackage[usenames]{color}
%\usepackage{textcomp}
%\usepackage{latexsym}		%fï¿½r Re(z) und Img(z)
%\usepackage{amsthm}
%\usepackage{wrapfig}
%\usepackage{pdfpages}
%\usepackage{caption}
%\usepackage{subfig}
\usepackage{listings}

%fuer TikZ zum Zeichnen von Graphiken
\usepackage{tikz}
\usetikzlibrary{chains}
\usetikzlibrary{fit}
\usepackage{pgflibraryarrows}		%optional
\usepackage{pgflibrarysnakes}		%optional
\usepackage{epsfig}

% Added by Raban
\usepackage{enumerate}  
\usepackage{nicefrac} 
\usepackage{braket}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{bm}  % Define \bm{} to use bold math fonts
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{verbatim}
\usepackage{natbib}
\makeatletter
\makeatother

\begin{comment}
%-------------- start insert modified commands ------------------
\makeatletter
\def\blx@bblfile@bibtex{%
 \blx@secinit
 \begingroup
 \blx@bblstart
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\input{biblio_MA.bbl}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \blx@bblend
 \endgroup
 \csnumgdef{blx@labelnumber@\the\c@refsection}{0}}
\makeatother
%-------------- end insert modified commands ------------------
\end{comment}


%\usetikzlibrary{external}
%\usetikzlibrary{decorations.pathreplacing}
%\tikzexternalize[prefix=ConfPaper-]


%\include{psfig}

\usepackage{listings}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{sidecap}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


\usepackage[latin1]{inputenc}

\usepackage{paralist} % for enumerate

\usepackage{mathabx} % for nice math symbols
%\usepackage{MnSymbol} % for nice math symbols

\usepackage{footmisc} % for \footref

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Macros:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for IEEE transaction submission it is not allowed to have a separate style-file. Therefore we have to copy the commands we used
\def\Hb{\ensuremath{H_{\rm b}}}
\def\S{\mathsf{S}}
\def\D{\mathsf{D}}
\def\C{\mathsf{C}}
\def\T{\mathsf{T}}
\def\E{\mathsf{E}}
\def\F{\mathsf{F}}
\def\R{\mathsf{R}}
\def\Z{\mathsf{Z}}
\DeclareMathOperator{\dec}{dec}
\DeclareMathOperator{\decp}{dec'}
\def\Rset{\mathcal{R}_\epsilon}
\def\Dset{\mathcal{D}_\epsilon}
\def\Bparam{Bhattacharyya parameter}

\DeclareMathOperator*{\argmax}{arg\,max}

% Comments
\newcommand{\RI}[1]{{\color[RGB]{0,128,0} *~*~*~ #1 *~*~*~}}

% QRM notaion
\newcommand{\ketbra}[1]{|#1\rangle\langle #1|}
\newcommand{\cnot}{{\footnotesize \textnormal{CNOT}} }
\newcommand{\RM}{\mathcal{RM} }
\newcommand{\QRM}{\mathcal{QRM} }

\newcommand*{\ee}{\mathrm{e}}


%\DeclareMathOperator{\T}{T}
\DeclareMathOperator{\enc}{enc}
\DeclareMathOperator{\EC}{EC}
\def\A{\mathsf{A}}
\def\H{H}
\def\P{P}
\newcommand{\+}{\textnormal{+} }
\newcommand{\bp}{\textbf{+} }
\newcommand{\g}{\hspace{1mm}}
\renewcommand{\d}{\textnormal{d}} %for integration measure
\newcommand{\id}{\textnormal{id}}
\def \I{\mathrm{i}}

% Joe notaion
\def\tr{{\rm tr}}
\def\pr{{\rm Pr}}
\def\Re{{\rm Re}}
\def\cl{{\text{cl}}}
\def\ker{{\text{ker}}}
\def\fpg{F_{{\rm pg}}}
\def\cC{\mathcal C}
\def\cD{\mathcal D}
\def\cE{\mathcal E}
\def\spec{\text{spec}}


\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\normT}[1]{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
    \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor Spe-ci-fi-cally di-mensio-nal}
\hyphenation{va-rie-ty}
\hyphenation{the-sis}
\hyphenation{opti-ma-li-ty}
\hyphenation{dif-fe-rent}
\hyphenation{ma-the-ma-ti-cal}
\hyphenation{pa-ra-me-ter}
\hyphenation{equali-ty}
\hyphenation{in-tro-duc-tion}
\hyphenation{anal-y-sis}
\hyphenation{in-equa-li-ty}
\hyphenation{using}
\hyphenation{ge-ne-ra-lized}
\hyphenation{ope-ra-tors}
\hyphenation{the-ory}
\hyphenation{pro-ba-bi-li-ty}
\hyphenation{in-te-re-stin-gly}
\hyphenation{multi-variate}
\hyphenation{exact}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SYNTAX PACKAGE ``theorem'':
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Example:
%%
%% \newtheorem{name}{titel}
%% \newtheorem{name}{titel}[counter] 
%% \newtheorem{name}[othername]{titel}
%%
%% The first version defines a new type of "theorem" that is invoked by
%%    \begin{name}
%%      ...
%%    \end{name}
%% and that will be called "titel" in boldface. The numeration starts
%% from 1 and goes up through the whole document.
%% The second version does the same, but the numeration is restarted at
%% each counter (where counter=chapter, section, subsection etc.),
%% i.e., for example 1.1, 1.2, 1.3, 2.1, 2.2, 4.1 according to the
%% chapters.
%% The third version puts the numeration to the same counter as the one
%% that has been defined for the "theorem" othername,
%% e.g., Lemma 1.1, Definition 1.2, Definition 1.3, Lemma 1.4 etc.
\newtheorem{mythm}{Theorem}[section]
\newtheorem{myprop}[mythm]{Proposition}
\newtheorem{mycor}[mythm]{Corollary}
\newtheorem{mylem}[mythm]{Lemma}
\newtheorem{myclaim}[mythm]{Claim}
\newtheorem{mysubclaim}[mythm]{Subclaim}
\newtheorem{myfact}[mythm]{Fact}
\newtheorem{myconj}[mythm]{Conjecture}

\theoremstyle{definition}
\newtheorem{mydef}[mythm]{Definition}
\newtheorem{myex}[mythm]{Example}
\newtheorem{myrmk}[mythm]{Remark}


% %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SYNTAX HEADER AND FOOTER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Attention: These headers and footers are only used if you use
%% \pagestyle{fancyplain} 
%%
%% Header and footer have three parts:
%% \lhead, \chead, \rhead bzw. \lfoot, \cfoot,\rfoot. 
%% \lhead ist responsible for TOP LEFT, \chead for the TOP MIDDLE etc.
%% Synopsis: \lhead[\fancyplain{1}{2}]{\fancyplain{3}{4}}
%% 1: Start of chapter on even page
%% 2. Normal even page
%% 3. Start of chapter on odd page
%% 4. Normal odd page 
%%
%% \leftmark = chapter
%% \rightmark = section with number
%% \thepage = page-number
%% \thechapter = chapter-number
%
%\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
%\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
%
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

%
\lhead[\fancyplain{}
{}]
{\fancyplain{}
  {}}
%
\chead[\fancyplain{}
{}]
{\fancyplain{}
  {}}
%
\rhead[\fancyplain{}
{}]
{\fancyplain{}
  {}}
%
\lfoot[\fancyplain{}
{}]
{\fancyplain{}
  {}}
%
\cfoot[\fancyplain{\thepage}
{\thepage}]
{\fancyplain{\thepage}
  {\thepage}}
%
\rfoot[\fancyplain{}
{}]
{\fancyplain{}
  {}}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PARTIAL COMPILATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% While working on a part of the document, compile only this 
%% part: 
%\includeonly{chapter1.tex}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% COUNTER OF FORMULAS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Counter including section-number (for more information see 
%% amsmath):
\numberwithin{equation}{chapter}
%\numberwithin{equation}{section}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% FRENCH SPACING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Doesn't put any additional space after interpunction-characters
%% like : , . 
%\frenchspacing
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% INDENT OF PARAGRAPH
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% After paragraph start sentence with indent:
%\setlength{\parindent}{0cm}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SIMPLIFIED TYPESETTING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% If you activate the simplified setting, then LaTeX uses less 
%% strict rules for the layout (less 'overful hbox' warnings)
%%
%% Activate:
%\sloppy
%% Deactivate:
\fussy
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PAGE-MARGINS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Left page more to the left, right page more to the right:
\setlength{\oddsidemargin}{1.7cm}
\setlength{\evensidemargin}{1.7cm}
\addtolength{\textheight}{0.0cm}
\addtolength{\textwidth}{0.0cm}
\addtolength{\topmargin}{-0.0cm}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% STYLE OF HEADER AND FOOTER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% NB: Order of this command and the Page-Margins commands is 
%% important!
%
\pagestyle{fancyplain}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SPACE BETWEEN LINES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Ph.D. should have 1.5 times bigger space between lines
%% Vieweg Latex Buch p. 33
\renewcommand{\baselinestretch}{1.2}
\large \normalsize
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% INDEX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Automated index, needs 
%% makeindex frame
%
%\makeindex
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% START OF DOCUMENT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{document}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% LANGUAGE OF DOCUMENT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\selectlanguage{USenglish}
%\selectlanguage{ngerman}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PAGENUMBERING PREFACE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Roman pagenumbering (arabic, roman,...)
\pagenumbering{Roman}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITEL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% AUTOMATICAL:
%
%\title{Eidgen\"ossiche Technische Hochschule \\ Z\"urich \\ \vspace{20mm}
 %Master's Thesis \\ at the Signal and Information Processing Laboratory \\
 %\vspace{10mm} The Hypothesis of Fixed-Key Equivalence for the Group
% Generalization of Linear Cryptanalysis \vspace{5mm}}
%\author{Stefan M. Moser}
%
%
%\maketitle
%
%% BY HAND:
%
\begin{titlepage}
  \mbox{}

  \vspace{-1.5cm}
  \noindent
  \begin{tabular}{@{} l @{} l @{}}
    \begin{minipage}[c]{0.5\textwidth}
      \hspace{-4mm}
      \includegraphics[height=19mm]{figures/eth_logo.pdf}
    \end{minipage} &
    \begin{minipage}[c]{0.5\textwidth}
       \hfill \large Quantum Information Theory Group
    \end{minipage} \\
  \end{tabular}
  \rule{\textwidth}{0.5pt}
  \begin{center}
    {\Large 
      Spring 2020 \hfill Prof.~Dr.~Renato Renner
    }
    
    \vspace{\stretch{5}}
    \LARGE
    Master's Thesis
 
    \vspace{\stretch{8}}
    \Huge\textbf{
    Designing experiments with neural networks
          }
    
    \vspace{\stretch{10}}
    \LARGE{
      Eduardo Gonzalez Sanchez
    }
    
    \vspace{\stretch{10}}
    \rule{\textwidth}{0.5pt}
   
    \vspace{0.0cm}
    \begin{flushleft}
      \begin{tabular}{ll}
        \Large Advisor: & \Large 
        Raban Iten
      \end{tabular}
    \end{flushleft}
  \end{center}
\end{titlepage}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% OFFICIAL PROJECT-DESCRIPTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% I do not have one ....
\thispagestyle{plain}
\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ACKNOWLEDGMENTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\include{preface}
%
%\renewcommand{\chaptername}{}
\chapter*{Acknowledgments}
%\addcontentsline{toc}{chapter}{\numberline{}Acknowledgments}
%\label{cha:acknowledgments}
\blindtext


\vspace{1.1cm}
\noindent
Zurich,  April 14, 2020

\vspace{2.4cm}
\noindent
Eduardo Gonzalez Sanchez
%\renewcommand{\chaptername}{Chapter}
\thispagestyle{plain}
\clearpage

\thispagestyle{plain}
\cleardoublepage
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ABSTRACTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ENGLISCH ABSTRACT:
%
%\include{englishabstract}
\huge
\begin{abstract}
  \setcounter{page}{5}
\thispagestyle{plain}
%  \addcontentsline{toc}{chapter}{\numberline{}Abstract}
  \normalsize
  \vspace{0.5cm}


\noindent (\textit{Mock version, not definitive}) In this work we propose a new
model that mixes reinforcement learning and deep learning to create agents able
to design strategies for experiments, using as feedback only the quality of the
predictions about properties of the physical system made exclusively from the
data the agents collect. This is could be summarized as 'agents able to do
science'; since science checks its validity by making predictions over the
physical world. In the thesis we also put the first building blocks of a
theoretical model for the scientific method in machines. This last part is
important in the long-term goal of developing a variant of quantum theory that
can consistently describe agents who are using the theory. If the science of the
future is done by machines, we must ensure consistency in the underlying
principles driving automated science.

\vspace{3mm}

\noindent 

\vspace{10mm}
\noindent{\bf{Keywords:}}
Reinforcement learning, deep learning, automated science, feature representation, 
experiment design, artificial intelligence
\end{abstract}



\normalsize


%\renewcommand{\abstractname}{Abstract}
\selectlanguage{USenglish}
%

%
\thispagestyle{plain}
\cleardoublepage
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% HEADER TABLE OF CONTENTS ETC.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TABLE OF CONTENTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\addcontentsline{toc}{chapter}{\numberline{}Table of Contents}
\tableofcontents
%\addtocontents{toc}{\protect\enlargethispage{1cm}}
\clearpage
%





\thispagestyle{plain}
\cleardoublepage



\lhead[\fancyplain{\scshape \leftmark}
{\scshape \leftmark}]
{\fancyplain{\scshape Semester Project}
  {\scshape Semester Project}}
%
\rhead[\fancyplain{\scshape Semester Project}
{\scshape Semester Project}]
{\fancyplain{\scshape \leftmark}
  {\scshape \leftmark}}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% MAIN PART
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% RESET NUMBERING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\setcounter{chapter}{0}
\setcounter{figure}{0}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PAGE LAYOUT MAIN PART
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\pagenumbering{arabic}
%
\renewcommand{\thechapter}{\arabic{chapter}}
\renewcommand{\thesection}{\thechapter.\arabic{section}}
\renewcommand{\thefigure}{\thechapter.\arabic{figure}}
%\renewcommand{\thefootnote}{\arabic{footnote}}
%
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
%
\lhead[\fancyplain{\scshape Chapter \thechapter}
{\scshape Chapter \thechapter}]
{\fancyplain{\textsc{\leftmark}}
  {\rightmark}}
%
\rhead[\fancyplain{\scshape \leftmark}
{\textsc{\leftmark}}]
{\fancyplain{\scshape Chapter \thechapter}
  {\scshape Chapter \thechapter}}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% MAIN CHAPTERS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\cleardoublepage
%\include{chapters}



\chapter{Preface} 

Many of the limitations of humans at doing science come from their biological
condition:
    \begin{itemize}
        \item  Humans have a limited lifespan: 72.6 years on average
        \cite{owidlifeexpectancy}. When experienced scientists die their
        knowledge and expertise die with them.
        \item From the limited lifespan, humans can dedicate only a fraction to
        do science. This is inevitable since humans need to eat, sleep and deal
        with social interactions, among other things. Moreover, humans need
        decades of study and training to start making contributions to the
        scientific knowledge.
        \item Humans are susceptible to suffer from diseases and other limiting
        biological conditions that hinder their scientific production.
        \item Humans' understanding of the physical world is tightly linked to
        their limited sensorial perception and other inherited or acquired
        factors like the language or the cognitive capacity.
    \end{itemize}
    
Other limitations are indirectly caused by the need to satisfy their biological
necessities. For example, a person who wants to dedicate their life to science
needs some type of financial support to satisfy the basic human necessities.
This support usually comes from a greater institution like a state, a company or
a patron. This financial relationship ties infrangibly science to the economical
structure of the society. Profitable discoveries are encouraged while resources
for unprofitable science are scarce. It can be argued that any form of
scientific research, human or not, will require an investment of energy and
resources in a society in which those are limited. This can be true, but a more
efficient way of doing science will increase science independence from the
economy. 

At the same time, scientific discoveries influence drastically modern society
and its economical structure. They provide new knowledge that allows humanity to
develop new tools and protocols to improve human well being. In the last
centuries, science has changed society by setting the theoretical and
experimental grounds of a technological transformation. It is of public interest
to boost and improve scientific production.

During the last century, the amount of available scientific literature has been
growing exponentially \cite{sinatra2015century,BornmannRudiger}, with a yearly
growth rate of $ {\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}} 9\%$ in the
last decade. Scholars read on average almost 240 articles per year
\cite{publications7010018}. Some authors \cite{Alkhateeb} suggest that science
is in the midst of a data crisis. Although the available literature grows
exponentially the cognitive capacity of human beings remains constant. This
forces scientists to derive hypotheses from an exponentially smaller fraction of
the collective knowledge. This will lead to scientists increasingly asking
questions that already been answered and reducing further the efficiency of
scientific production. 

Some areas of science are starting to suffer from a reproducibility crisis
\cite{Saltelli, begley2012raise} in which scientists are generally unable to
reproduce their peers' findings. Some voices in the physics community
\cite{Sabine} point out that foundational physics has been stagnated during the
last decades. However, some authors defend that there isn't such a crisis
\cite{Fanelli2628}. Nonetheless, it is clear that to sustain an exponential
growth of reliable scientific production with no exponentially increasing human
effort is impossible, and the crisis is thus, unavoidable.
 \par
However, the lack of efficiency in scientific production is not the only
drawback produced by the biological limitations of human beings. Humans'
intuition and understanding of the physical world is conditioned by the percepts
collected by their sensory system. This limitation becomes evident when trying
to intuitively understand physical systems that show behaviors that differ from
those susceptible to be collected by the sensory system. This is the case of,
for example, quantum theory. Humankind has developed tools to overcome the
limitations of the sensorial system to observe new properties of physical
systems that are out of reach for our biological receptors. For instance, using
infrared cameras to map infrared signals to a representation in the visible
spectrum, humans can detect infrared radiation. But these tools don't allow to
build an intuitive understanding of the phenomena without analogies to the
phenomena perceived by the sensory system. For example, people that are blind
from birth have never had any input to their visual cortex, so they have no
visual intuition which limits their ability to understand some physical
concepts. Similarly, the lack of receptors for other arbitrary physical
properties limits human understanding of the physical world and likely hinders
scientific advance. \\ 
\par 
Modern science requires from agents with complex cognitive abilities. So far
humans are the only known material structure able to perform it. It is true that
some animals perform scientific behavior, like Crows or monkeys solving puzzles
by trial and error. But those anecdotal examples are far from the formalized
version of the scientific method employed by humans. However, humans are also
the living f of the possibility of agents performing sophisticated science.
There is no reason to think that there's anything special in humans that makes
them the best possible form of a scientific agent. Rather it is reasonable to
think that there is plenty of space for improvement, since the human brain was
designed solely by millions of years of random mutations and natural selection.

Recent advances in artificial intelligence, yet far from achieving an artificial
general intelligence, open the door to an automation of science. In the recent
years, a vast amount of effort has been dedicated to the development of machine
learning techniques to help scientists of the physical sciences to process data
to create new better models \cite{Carleo_2019}. However, these machine learning
based techniques are just tools to help human scientists to interpret complex
data to provide new predictions, and not efforts towards an automation of
science. Nonetheless, the potential role that artificial intelligence might play
in the process of scientific production has been getting growing awareness. In
\cite{Melnikov_2018}, the authors use a projective simulation model to design
complex photonic experiments that produce high-dimensional entangled multiphoton
states. According the authors, the system autonomously discovers experimental
techniques which are a standard in modern quantum optical experiments. In
\cite{iten2020discovering} the authors explore the use of variational
autoencoders to extract autonomously physically relevant parameters from
physical data without prior assumptions about the physical system. In
\cite{nautrup2020operationally} they expand the work to present an architecture
based on communitcating agents that deal with different aspects of a physical
system and show that it can be combined with reinforcement learning techniques.
More work on similar directions can be found in [citas articulo de Raban II].
\par
However, scientific agents need to be designed carefully to minimize the
inherited biases and limitations from their human creators. In this thesis we
present a minimal axiomatic model for science and a new model architecture that
mixes reinforcement learning with deep learning to create agents capable to
design strategies for experiments. Using as feedback only the quality of the
predictions about properties of the physical system made exclusively from the
data the agents collect from their sensorial available receptors. 
\\
\\
\textit{Here I will write a couple of paragraphs explaining the structure of the 
thesis and pointing out to the repository, where to find the code, etc.}

\vspace{12mm}


% \chapter{Minimal model for science} \label{science}

% \section{Introduction} \label{sec:intro_science}
% In this section, we introduce a minimal set of definitions and assumptions to
% define a scientific method. In the goal of achieving an independent automated
% science protocol, we must ensure consistency in the underlying principles to
% avoid unwanted biases and preconceptions inherited from humans.
% \subsection{First assumption: Dynamicality}
% We start with a universe $U$. Since the goal is to create agents able to
% decipher the properties of the universe $U$, we must make the minimal number of
% assumptions that allow us to set a scientific method. First, we need the
% universe to be dynamical. If the universe is static, nothing changes and no
% science is possible. This will give use the first assumption:

% \begin{itemize}
%   \item[\textbf{A1 (Dynamicality)}:] There exists a dynamical universe $U$.
% \end{itemize}

% We can represent the dynamical nature of the universe by parametrizing it with a
% real parameter $\tau \in (-\infty,\infty)$, so that the state of the universe is
% a function of $\tau$. Note that we are not assuming any property of the universe
% function $U(\tau)\rightarrow S$, where $S$ is the set of possible states of the
% universe. For example, it may look that by setting the parameter $\tau$
% unbounded we may be forcing the universe $U$ to have unbounded dynamics.
% However, we could have $U(\tau)$ so that:
% \begin{align*}
%   U(\tau \leq \tau_{\textnormal{initial}})&=s_\text{initial} \\
%   U(\tau \geq \tau_{\textnormal{terminal}})&=s_\text{terminal}
% \end{align*}
% where $s_\textnormal{initial}, s_\textnormal{terminal} \in S $ are the initial
% and terminal states of the universe $U$. We aren't making any assumptions about
% the dynamical bounds on $U$. Also, we aren't making any assumptions on any other
% properties of $U(\tau)$ or even on what the elements of $S$ are. We aren't also
% making any assumption on the continuity of the dynamics, since we could have:
% $$U(\tau_{i} >\tau \geq \tau_{i+1})=s_{\tau_i},\;\;\forall i \in \mathbb{N}$$
% where $\{\tau_i\}_{i=0}^\infty$ is an arbitrary monotonically increasing
% sequence of real numbers. We aren't assuming as well anything about the
% deterministic nature of $U$, since $U(\tau)$ could be a probabilistic function.
% The only assumption made by the statement \textbf{A1} is that the universe
% evolves according some rule $U(\tau)$.

% Note that the parameter $\tau$ doesn't necessarily represent the time as
% perceived by humans. It's just a parameter defined to convey the dynamical
% nature of the universe.

% \begin{myex}
%   \label{example1}
% In this example we are going to define a universe that satisfies \textbf{A1}.
% The universe $U$ consists of $n^2$ elements $\{a_{jk}\}_{j,k=0}^{n-1}$. Each
% element can exist in one of two substates: \{1, 0\}. The set of states $S$ is
% then $\{0,1\}^{n^2}.$ Now we need to equip the universe with a dynamical law
% $U(\tau)$. Assuming a discrete evolution, it could be, for example, the laws of
% a deterministic cellular automata. Or a probabilistic law so that each element
% changes its state with a certain probability in each dynamical step. It could be
% any rule that associates a state of $S$ with each discrete value $\tau_i$.
% However, we could also have continuous dynamics. For instance, we can set:

% \begin{equation}
%   P(a_{jk}=1,\tau)=e^{-\tau^2} ,\;\;\forall j,k 
%   \label{DynamicalLaw1}
% \end{equation}

% where $P(a_{jk}=1, \tau)$ is the probability of the element $a_{jk}$ being in
% the state $1$ at the \textit{time} \footnote{For communicative convenience we
% use the word time to design the value of $\tau$. However, it doesn't mean that
% $\tau$ represents the time as perceived by humans.} $\tau$. With this dynamical
% law, the evolution of the universe is unbounded, although it has terminal and
% initial states: all elements in the substate 0. One may ask what it means that
% the universe is described by a probability function like \eqref{DynamicalLaw1}.
% It means that at a given value of $\tau$ the state of the universe is choosen
% randomly according to \eqref{DynamicalLaw1}.
% \begin{figure}[t]
%   \centering
%   \includegraphics[width=100mm,scale=0.5]{figures/UniverseA1.pdf}
%    \caption{Graphical representation of the dynamical change of a universe with
%    $n=8$.}
% \end{figure} 
% \end{myex}

% From the assumption \textbf{A1} we can deduce some consequences.
% \begin{myclaim}
% Any universe that satisfies \textbf{A1} has at least one element that can exist
% in more than one substate.
% \end{myclaim}

% The f of this claim is obvious: an empty set cannot change. A set in which
% all elements can exists in only one substate also cannot change. Therefore, the
% simplest universe in which \textbf{A1} holds is a universe with only one element
% that can exists in two substates, this is, a bit that changes its value
% according to a dynamical law.

% \subsection{The scientific method}

% Now we need to define what is science in this minimal context. First, let's
% define what an agent is.

% \begin{mydef}
%   An agent $A$ is defined as a subset of the universe $U$. $A(\tau)$ is the
%   composition of $A$ at the dynamical value $\tau$. The dynamical evolution of
%   $A$ is determined by the dynamical evolution of $U$.
% \end{mydef}

% This is a very broad definition of an agent, since we define them just as
% subsets of the universe, so anything can be an agent. We can safely make this
% assumption since humans and computers are subsets of the universe. The
% definition implies that agents obey the same physical laws than the universe.
% Some philosophers would call this implication a materialistic assumption.

% \par

% Another definition required for science is the definition of measurement or
% observation. This definition is particularly delicate in the context of quantum
% theory, so we have to be very careful in its definition. 

% \begin{mydef}
%   An observation $\hat{O}_A$ is defined as the dynamical process in which the
%   state of any subset of an agent $A$ gets correlated to the state of another
%   subset of $U$, the object $O$, that may or not be disjoint to $A$.  
% \end{mydef}

% This definition is also very broad, so let's explain it. When we talk about
% observations in the context of humans, we usually understand them as information
% from an object acquired by our sensorial system, for example by observing with
% our visual system a bunch of photons emitted from a source of light. However,
% these terms mean nothing in our minimal model so we need to be more specific.
% When we see an object, the process, as far as we know, happens because a photon
% coming from the object hits some receptors in our retina producing a chain
% reaction that triggers an specific state in some part of the brain. In other
% words, an specific part of us (the agent $A$) gets correlated to the state of
% the object (a subset of $O$ the universe) as a result of the dynamical evolution
% of the universe. The dynamical process that gets both states correlated is an
% observation.

% \begin{myex}
%   In this example we are going to see how an observation translate to our simple
%   model of universe. With the same universe than in \ref{example1}, we can
%   define a new dynamical law that consists on a 1 doing a random walk in a
%   bidimensional grid of zeros. Each time an element $a_{jk}$ switches to 1, it
%   gets correlated to the rest of the elements of the universe since all of them
%   must be zero. In this case, the agent $A$ is the subset of $U$ containing only
%   $a_{jk}$ while the object $O$ can be any subset of $U$. We say then that $A$
%   has made an observation $\hat{O}_{A}$.
% \end{myex}

% We need one more definition to define the scientific method. Science is about
% making predictions about the physical world, so we need to define what a
% prediction is in our minimal context.



% \begin{mydef}
%   A prediction is... \textit{(I haven't come yet with a successful definition
%   for a prediction. I'm trying to formulate it with different agents trying
%   to communicate correlations about the "future" of some subsystem of U)}
% \end{mydef}

% \subsection{Second assumption: Emergency}
% \textit{I want to have a formal definition of predictions and the scientific
% method before writing this subsection. But it's just a corollary of the
% anthropic principle: if we are able to do science, then the dynamical law of the
% universe must allow for scientific agents to be generated. For example, a simple
% cellular automaton that converges to a stable state wouldn't fulfill this
% assumption.}

\chapter{Machine Learning theory} 

TO BE WRITTEN.

\chapter{Experimenter-Analyzer model} \label{EAmodel}

In this section, we are going to present our proposal for a basic model for a
machine learning set-up that designs an experiment applying the scientific
method. It is a model that mixes reinforcement learning and and deep learning to
create agents able to design strategies for experiments, using as feedback only
the quality of the predictions about properties of the physical system made
exclusively from the data the agents collect. 

One of the cornerstones of our model is the principle of modularity: in order to
be applicable to a wide range of set-ups it needs to be constituted by simple
individual elements that can be combined to build complex learning loops. The
agent that will learn to design experiments and make predictions with them is
composed by two different kinds of sub-agents that complement each other:

\begin{itemize}
  \item \textbf{Experimenters}: this kind of sub-agents are reinforcement
  learning based agents. Its goal is to take decisions about influencing the
  dynamics of the physical system (e.g by modifying a parameter of a controlled
  experiment) or about what measurements to make (eg. by choosing what property
  of the physical system to measure). It could be that the choice of
  measurement modifies as well the dynamics of the physical system (eg. by
  choosing which slit to "look at" in the double slit experiment). To choose
  what actions to take they can consider previous observations and other kind of 
  information.
  \item \textbf{Analyzers}: this kind of sub-agents are learning agents
  that take the information gathered by the Experimenters to make predictions
  about the environment. Then, based on the accuracy of those predictions
  compared to the real values \footnote{One may ask how these real values are
  obtained. Those values could be obtained by measurements made by other
  Experimenters that form part of the learning agent. However, for simplicity in
  the work presented here we assume that the real values needed to check the
  validity of the prediction are always accessible with unlimited accuracy to
  the agent. In the context of this modular framework we could assume that we
  have an Experimenter that its only possible action is to measure the real
  value of the target property.} of the environment a reward is generated to
  train all the sub-agents. 

\end{itemize}


\begin{figure}
  \centering
  \includegraphics{figures/SimpleSetUp.pdf}
  \caption{Diagram of the simplest combination of elements. The feedback loop
  consists on the following: 1. The Experimenter takes an action. 2. The
  environment gives back an outcome resulting from the action. 3. The action is
  taken by the analyzer and used to make a prediction. 4. The prediction about
  the environment and the real value of the environment are compared to generate
  the loss and the reward to train the Analyzer and the Experimenter.}
  \label{fig:simplesetup}
\end{figure}

To get a better understanding of the purpose of each part of the model let us
take a look to the simplest structure that can be formed with the model (Fig.
\ref{fig:simplesetup}). To illustrate it let us imagine a simple physical set-up.
Suppose we have a simple gravity pendulum of length $L$ and mass $M$ and the
goal is to find the oscillation period on Earth's surface. We know the period is
a function only of the length $L$ and the value of the Earth's gravity (which we
assume to be constant): $T \approx 2\pi\sqrt{\frac{L}{g}}$, so the value of the
mass $M$ is irrelevant for the purpose, but in principle this is unknown to the
agent. In this set-up we give the Experimenter two possible actions to take (but
only one try): either measuring the mass $M$ (eg. by using a calibrated spring)
or measuring the length $L$ of the string (e.g by using a calibrated ruler).
When the Experimenter takes an action over the Environment, it produces an
outcome (either the mass $M$ or the length $L$). This outcome is passed to the
Analyzer which tries to make a guess of the period based on the value of the
outcome of Experimenter's action. Then the real period of the pendulum is
observed and compared to the value predicted by the Analyzer. Depending on the
result of the comparison a loss and a reward are generated to train the
Experimenter and the Analyzer. In general: the closer the value of the
prediction to the actual value the smaller the loss and the higher the reward.
After the iteration is completed, a new pendulum with new values of $M$ and $L$
(ideally generated uniformly at random under the i.i.d. assumption) is generated
and the process starts again.

The goal in the set-up depicted in Fig. \ref{fig:simplesetup} is to form a
feedback loop between the Experimenter and the Analyzer from the, at first
unknown, correlations between the outcomes and the values to be predicted. The
better the Experimenter gets, the better the data available for the Analyzer to
make better predictions that will generate better rewards for the Experimenter.
Hopefully, this feedback loop will converge to an optimal experiment strategy
and an optimal model for the predictions. At the beginning both sub-agents will
start by giving random outputs since they are not trained. This is what we call
the \textit{exploration phase} and it is of fundamental importance for the
feedback loop to start. It can be artificially enforced, for example with
$\epsilon$-greedy algorithms that we will discuss later in this work. If there
exists any correlation between the choices of the Experimenter and the target
value to be predicted, the Analyzer's training algorithm (usually Stochastic
Gradient Descent) can exploit those correlations to start the descent to some
local or global minimum. This working principle is somehow similar to the use of
artificial neural networks for feature selection in classical machine learning
theory.

The alert reader may have noted that in \ref{fig:simplesetup} the Analyzer has
no apparent way of telling which action the Experimenter took, since it only
receives the outcome of the experiment. Therefore the Analyzer doesn't know if
what is receiving is the value of $M$ or $L$. Although it may appear counter
intuitive, the Analyzer doesn't care in this particular set-up. The optimal
strategy is to treat everything as if it were $L$ since knowing the value of $M$
doesn't provide any information about $T$. However, it may make sense for the
Analyzer to know when $M$ is provided if the objective is to minimize the loss
function (e.g. if the Analyzer knew that the value corresponds to $M$ the
optimal prediction would be the random guess that minimizes the loss). In
practice, unless the distributions where $M$ and $L$ are generated from are the
same, the Analyzer learns to differentiate them with high confidence just by
value of the outcome. However, in other set-ups this may not be the case and the
information about what actions were taken is useful for the Analyzer to make the
predictions. In this case, we should also feed the actions as inputs to the
Analyzer. As we will see, these Experiment-Analyzer loops can grow very quickly
in complexity when several sub-agents are involved. This is why we introduce the
concept of \textit{buffer}.

In our model we can use a buffer to store everything that we might want to feed
to the agents. Usually this will be the actions taken and the outcomes obtained
in every observation of the environment, although we have the freedom to add
whatever we might need. It is just a variable to store information. This
variable is not strictly needed, since we could establish directly the
connections between the different subagents and environment outcomes. However,
when we have a few sub-agents with several outcomes it becomes very difficult to
keep track of each connection and to represent them. In these cases, the concept
of buffer becomes highly useful from an illustrative point of view. Usually, the
buffer is emptied at the end of each episode or iteration of the learning loop.
We could not do so, but in practice, keeping the data in the buffer doesn't
provide any easy advantage since to use it we should solve many challenges, for
example an increase on the number of features fed to the agents each episode.
We won't refer the buffer as \textit{memory} in this work, since the word
\textit{memory} is reserved for an element of some versions of sub-agents that
doesn't restart in each iteration. 


\begin{figure}
  \centering
  \includegraphics{figures/SimpleSetUp(Buffer).pdf}
  \caption{Diagram of a simple learning loop with buffer. In this case, the
  process is identical to the process depicted in Fig. \ref{fig:simplesetup}
  with the difference that the values of the action and the outcome are stored
  in the buffer, and then passed to the analyzer. In this case the information
  about which action was taken is available to the Analyzer.} 
  \label{fig:simplesetupbuffer}
\end{figure}

\section{General elements and definitions}
In this section let us define more formally some of the elements and concepts of
our model.
\begin{itemize}
    \item \textbf{Simulated physical environment (SPE): } it represents the
    physical world that determines the outcomes of all the measurements realized
    by the agent. It is completely represented by a state $ s \in S $, where $S$
    is the set of all possible states in which the SPE can be. For this
    particular model we assume that the evolution of the system is assumed to be
    given by a Markovian function $f: S \rightarrow S$ such that
    $f(s_\tau)=s_{\tau+1}$. This function maybe deterministic or not.
    \item \textbf{Agent:} the agent is formed by the composition of two kind of
    different subagents:
    \begin{itemize}
        \item \textbf{Experimenter:} this part has the task of making decisions
        that will have an effect on the observations. This effect could be due
        to the influence of the decisions in the evolution of the system (for
        example, if the agent decides to shoot a bullet to a box with a certain
        velocity) or due to the fact that the decisions themselves could be what
        observations to make (for example, choosing to measure the position of a
        box within a spatial range). This agent will be implemented using
        reinforcement learning techniques. Mathematically it can be represented
        by a trainable function $E:\mathcal{X}\rightarrow\mathcal{A}$ where
        $\mathcal{A}$ represents the space of possible actions and $\mathcal{X}$
        is an arbitrary space that can represent any accessible information that
        might be useful to make a choice of action. In the context of
        reinforcement learning, $\mathcal{X}$ is the set of states of the
        sub-agent's environment\footnote{Not to be confused with the SPE.}.
        \item \textbf{Analyzer:} the goal of this part of the agent is to
        process the data collected in the observations after the action of the
        Experimenter to predict a physical property of the SPE. This property
        could be a physical parameter of the system or a prediction of the
        dynamics of the environment. It will usally consist of a regular
        regression neural network or an autoencoder, depending on the specific
        set-up. But we could use any trainable function like SVMs or any kind of
        regression. Mathematically it can be represented by a trainable function
        $ A :\mathcal{M} \rightarrow \mathcal{P}$, where $\mathcal{M}$ is the
        space of measurements and $\mathcal{P}$ is the space of predictions.
  \end{itemize}
    \item \textbf{Orchestration:} when coding we will call Orchestration
    everything that connects the different parts to complete a feedback loop.
    For example, one of the tasks of the Orchestration is to transform the state
    of the SPE into the measurements received by the Analyzer. The Orchestration
    is needed when we simulate fictitious physical situations in a computer to
    test our models. It represents somehow something analogous to the sensors
    and wires that connect the external environment to the processor of an
    autonomous robot.
\end{itemize}

\begin{myex}
Let us complete the example of the pendulum to fully illustrate the architecture.
First, we define the SPE. Let us assume that the mass of the pendulum and the
length of the string are sampled uniformly at random from the intervals \break
$0.1$ kg $< M < 1$ kg and $ 0.1$ m $<L<1 $ m. The space of states of the SPE
is then:
 \begin{equation}
  S\equiv \{(m,l):m \in \left[0.1, 1\right],l \in [0.1,1]\}
 \end{equation}
Now, let us define an Experimenter. In this case, the Experimenter's environment
space $\mathcal{X}$ is the empty set $\emptyset$, since there is no input. This
means that the agent takes the action based solely on the rewards obtained. The
action space $\mathcal{A}$ is just $\{0,1\}$, with 0 representing the action of
measuring the mass and 1 the action of measuring the length.

We can therefore set a very basic decision rule for our Experimenter agent:

\begin{itemize}
  \item Let $\textbf{Q}=(q_0,q_1)$ be the value table, where $q_0$ and $q_1$ are
  the values associated with the actions 0 and 1 respectively.  $Q$ can be also
  understood as a trainable function $Q:\mathcal{X}\times\mathcal{A} \rightarrow
  \mathbb{R}$. Initially $Q(a)=0 \; \forall a \in \mathcal{A}$. 
  \item  Every episode, the agent takes an action $a$ and receives a reward $r$
  after taking the action.
  The update rule for $\textbf{Q}$ is the following:
  \begin{equation}
    q_a\leftarrow q_a + r
    \label{update_rule1}
  \end{equation}
  This is, the value $q_a$ is just the cumulative reward obtained by the 
  action $a$. 
  \item The agent's policy $E:\emptyset  \rightarrow \mathcal{A}$ or 
  decision rule is:
  \begin{equation}
    a = \argmax_{x \in \mathcal{A}} Q(x)
    \label{argmax_policy}
  \end{equation}
  This is, the Experimenter takes the action $a$ with the highest $q_a$, and 
  $E$ is trained by updating $\textbf{Q}$ each episode.
\end{itemize}

Note that the choice of \eqref{update_rule1} is just to illustrate that we can
build Reinforcement Learning agents with very simple rules. We could have
chosen instead of the cumulative reward the average reward obtained with the 
action, but the working principle would be identical.

However, our policy is still flawed, since the first action that gets a reward
would always be elected afterward independently of its optimality. To solve this
we can force the agent to initially explore different options ignoring the
decision rule, and slowly, when the values $q_a$ are more reliable, let the
agent choose according to \eqref{argmax_policy}. The easiest way to achieve that
is with an $\epsilon$-greedy decision algorithm: we set an exploration rate
$\epsilon$ that decreases in each episode until it reaches a minimum value
$\epsilon_\text{min}\geq 0$. The Experimenter is set to take the
\eqref{argmax_policy} policy with probability of $P_{\text{greedy}}=1-\epsilon$
and a random action with a probability of $P_{\text{random}}=\epsilon$. We have
the freedom to choose how to decrease the value of $\epsilon$, and it may have a
crucial impact on the training. A very popular rule is to decrease the value
constantly by subtracting $_\epsilon = 1/N$, where $N$ is the number iterations
of the training. So in each episode:
\begin{align}
  &\text{if}\; \; \epsilon > \epsilon_\text{min} \;\;\; &\epsilon&\leftarrow\epsilon - _\epsilon \\
  &\text{else} \; \;  &\epsilon &\leftarrow \epsilon_\text{min}
\end{align} 


For the Analyzer, we set a trainable real function $A: S \rightarrow
\mathbb{R}$. We can use any trainable function, but to keep it simple let's use
a regular feed-forward neural network with one hidden layer of two neurons,
trained with the Stochastic Gradient Descent (SGD) algorithm and a the Mean
Squared Error (MSE) loss.

We need now to define how to calculate the reward.  In our case we want the
reward function to be high when the Analyzer predicts a period $T_\text{pred}$
close to the real period of the pendulum and low when the period prediction is
far. So, ideally we would like a function that:
\begin{align}
       r(T_{\text{pred}} \approx T) &\approx 1\\
       r(T_{\text{pred}}>> T \text{ or } T_{\text{pred}} << T) & \approx 0
\end{align}
One natural way to achieve this is using a Gaussian distribution over the
relative error $\Delta=(T-T_{\text{pred}})/T$. This is:
\begin{equation}
   r(\Delta)=\exp\left(-\frac{1}{2}\left( \frac{\Delta}{\sigma} \right) ^2\right)
  \label{gaussian_reward}
\end{equation}
and $\sigma$ allows us to modify the precision we want in order to achieve a 
reward.

Now we have every element defined. Let us outline the full algorithm in 
pseudocode:

  \makeatletter
  \def\BState{\State\hskip-\ALG@thistlm}
  \makeatother
  \begin{algorithm}
    \caption{Simple learning loop}\label{pendulum}
    \begin{algorithmic}[1]
    \Procedure{Pendulum}{}
    \BState count = 0
    \BState $\textbf{while } \text{count} < N:$
    \State $ M, L \gets \text{random sample }  M, L$
    \State $ T \gets T(L)$
    \State $ \text{pendulum} \gets [M,L,T]$
    \State $ \text{action} \gets \argmax Q $
    \State $ \text{measurement} \gets \text{pendulum[action]}$
    \State $ \text{prediction} \gets \text{analyzer(measurement)}$
    \State $ \text{Apply SGD to Analyzer with data point } (\text{measurement},T)$
    \State $ \text{reward} \gets \text{reward}(\text{prediction}, T)$
    \State $ Q[\text{action}] \gets Q[\text{action}]+ \text{reward}$ 
    \State $ \text{count }\gets \text{count} + 1$
    \EndProcedure
    \end{algorithmic}
    \end{algorithm}
\end{myex}

Let us see how this simple algorithm performs in the task. Ideally, the
Experimenter would find the optimal policy which consists in choosing always to
measure the length and the Analyzer would learn to estimate accurately $T$ from
the data obtained by the Experimenter. In our experiments, with this simple
structure of only two neurons and the very naive reinforcement learning policy
the agent consistently finds the optimal action (to measure $L$). It also
accurately predicts the value of $T$ in less than 10,000 episodes. With less
episodes it is less consistent in finding the optimal policy.

\begin{figure}[h]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/Avg-Action-Pendulum.pdf}
    \caption{Moving average of the actions}
    \label{fig:ActionAveragePendulum}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/Reward-Pendulum.pdf}
    \caption{Moving average of the reward}
    \label{fig:RewardAveragePendulum}
  \end{subfigure}


  \caption{(a) The moving average of the actions taken in each episode. We can
  see how it starts taking both actions evenly with an average of 0.5. This is
  expected since at the beginning $\epsilon \approx 1$. As $\epsilon$ decreases
  the Experimenter starts to take more greedy actions and gets biased towards
  the action 1 (measure $L$). This bias appears as a result of the existing
  correlation between $L$ and $T$, but not between $M$ and $T$. This correlation
  is exploited by the SGD algorithm for the neural network of the Analyzer. (b)
  The moving average of the reward obtained in each episode. At the beginning
  the reward obtained is low and similar to those obtained with random guesses.
  However, it increases slowly, apparently because the Analyzer starts to
  exploit slightly the correlation between $L$ and $T$. This slight improvement
  over the prediction for the outcomes of Action 1, creates the bias that starts
  the feedback loop. Values: $\sigma=0.05$, $N=10000$,
  $\text{lr}_\text{Analyzer}=0.01$, Optimizer = Adam}
  \label{fig:PendulumTraining}
  \end{figure}

  In the figures Fig. \ref{fig:PendulumTraining} and Fig.
  \ref{fig:PendulumError} we can observe the evolution of different parameters
  during the training. All the parameters show successful learning of both, the
  optimal policy and the prediction of the period. More complex neural network
  structures and longer trainings can achieve much lower errors. However, this
  is not a surprise since once the optimal policy is obtained the process just
  consists on fitting a continuous single variable function. A task that can be
  solved optimally by a single layered sigmoidal neural network
  \cite{cybenko1989approximation}. This pendulum example, although trivial, is
  useful to appreciate the working principle behind the Experimenter-Analyzer
  architecture and how the feedback loop is formed. In the next section we are
  going to explore less trivial examples with more complex learning loops.

  
\begin{figure}[t]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/Real-vs-Pred-Pendulum.pdf}
    \caption{Real T vs Predicted T}
    \label{fig:RealvsPredictedPendulum}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/Error-Pendulum.pdf}
    \caption{Moving average of the error}
    \label{fig:ErrorAveragePendulum}
  \end{subfigure}
  \caption{(a) A sample of the predictions made by trained agent. We can see
  how it fits the $y=x$ line, showing that it learned the relation between
  $L$ an $T$. (b) In this figure we can see how the error decreases to almost 
  zero with very low deviation.Values: $\sigma=0.05$, $N=10000$,
  $\text{lr}_\text{Analyzer}=0.01$, Optimizer = Adam
  }
  \label{fig:PendulumError}
  \end{figure}

\chapter{E-A architecture in different scenarios} 
\label{Scenarios}
\section{Scenario 1: Modified multi-armed bandit}
In the first section of this chapter we are going to study a modified version of
the classical Multi-armed bandit problem, a classical problem of Reinforcement
Learning and Probablity Theory. Let us start by introducing the general problem.
% We will show that we can map any experiment 
% consisting of a non-interactive discrete choice measurements to our modified
% multi-armed bandit. 
\subsection{General multi-armed bandit problem}
\begin{figure}[t]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/Bandit.pdf}
    \caption{Simple diagram portraying the multi-armed bandit problem}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/Bandit-Draw.pdf}
    \caption{Draw of a two armed bandit}
  \end{subfigure}
  \caption{Multi-armed bandit problem}
  \end{figure}

Suppose that we have an agent that has access to a k-armed bandit. Each time the
agent pulls one of the $k$ levers, the bandit produces a reward. The objective
of the agent is to maximize the obtained reward in a determined number of trials
or episodes $N$. The agent doesn't know how the bandit produces the rewards. Its
only way of getting information about the reward system is by trial and error by
pulling the levers and obtaining rewards.

Mathematically we can model our agent as an Experimenter
$E:\mathcal{X}\rightarrow\mathcal{A}$, where the action space is
$\mathcal{A}=\{0,1,...,k\}$ and the state space $\mathcal{X}$ is again the empty
set $\emptyset$. The bandit can be modelled as a probabilistic function
$R:A\times ... \rightarrow \mathbb{R}$, with "$...$" meaning that in principle
the generated reward can depend on anything: the previous actions, the previous
rewards, the number of trials, the number of times a lever has been pulled, etc.
This is an extremely general problem. Often authors make assumptions on $R$. For
example, $R$ being Markovian if depends only on the last rewards and/or actions.
Or stationary if $R$ can be modeled as a set of immutable distributions
$R=\{R_1,... , R_k\}$, with each distribution being associated with the rewards
delivered by each of the arms of the bandit.

We define the value $Q^*(a)=q^*_a$ of an action $a \in \mathcal{A}$ as the true
expected or mean reward associated to the action. We denote by $a_n$ the action
taken in the $n^\text{th}$ episode, and $r_n$ the reward obtained. This is:
\begin{equation}
  Q^*(a) = \mathbb{E}[r_n|a_n = a]
\end{equation}
Note that, although we don't write it explicitly, $Q^*(a)$ can also depend on 
any of the subsets of the domain of $R$.
The optimal strategy for the k-armed bandit problem is to always choose 
$a^*=\argmax Q^*(x)$.

Most reinforcement learning approaches to this problem look for an estimate of
$Q^*(a)$, $Q(a)$, and base their choice on $a=\argmax Q(x)$. In general, the
best algorithm depends deeply on the characteristics of $R$, e.g., an algorithm
that works well for stationary bandits can perform very poorly on non-stationary
bandits. The interested reader can read a gentle introduction to the multi armed
bandit problem in the Chapter 2 of \cite{sutton2018reinforcement}. We are not
going to discuss here many of the approaches to solve the multi-armed bandit
problem. However, let us outline a very simple algorithm that works particularly
well for non-stationary bandits. 

\subsection{Simple Reinforcement Learning algorithm for non-stationary
multi-armed bandit} \label{SimpleRL}

Let us outline a simple algorithm that we will use later in this section. This
algorithm can be understood as a particular case of the Q-Learning algorithm,
where the state space of the experimenter $\mathcal{X}$ is the empty set
$\emptyset$ and the discount factor $\gamma$ is set to zero. The update rule for
the estimate $Q(a)$ each time the action $a$ is taken is:

\begin{equation}
  Q(a) \leftarrow Q(a) + \alpha \left( r - Q(a) \right)
  \label{simpleQ}
\end{equation}

where $\alpha \in (0,1]$ is the called the learning rate, $Q(a)$ is the
estimate value for the action $a$ and $r$ is the reward obtained after taking
the action $a$.

If we name by $Q_n$ the estimate of $Q^*(a)$ for an arbitrary action $a$ at the
 $(n)^\text{th}$ time the action $a$ was taken, then due to \eqref{simpleQ} we
 have that:

 \begin{align}
  Q_{n+1} &=Q_{n}+\alpha\left(r_{n}-Q_{n}\right) \\
  &=\alpha r_{n}+(1-\alpha) Q_{n} \\
  &=\alpha r_{n}+(1-\alpha)\left[\alpha r_{n-1}+(1-\alpha) Q_{n-1}\right] \\
  &=\alpha r_{n}+(1-\alpha) \alpha r_{n-1}+(1-\alpha)^{2} Q_{n-1} \\
  &=(1-\alpha)^{n} Q_{1}+\sum_{i=1}^{n} \alpha(1-\alpha)^{n-i} r_{i}
  \label{weighted_sum}
  \end{align}
The last equation, \eqref{weighted_sum}, is just a weighted average of the
rewards obtained, where the weights give exponentially less importance to
rewards coming from distant actions in the past. Sometimes it is known as the
\textit{exponential recency-weighted average}.

\begin{myclaim}

  If $\alpha$ is constant on $n$, the exponential recency-weighted average is
  also a convex sum of the rewards obtained.
  \\
\end{myclaim}
  \textit{Proof:} Since $\alpha \in (0,1]$ all the coefficients are no greater
  than 1. We just need to prove that $\forall n$ they sum to one. If we write 
  the sum $S_n$ of the coefficients at the time $n$:
  \begin{equation}
    S_n=(1-\alpha)^{n} + \sum_{i=1}^{n} \alpha(1-\alpha)^{n-i}
    \label{eq:sum}
  \end{equation}
  We also have that: 
  \begin{align}
    (1-\alpha)^n &= (1-\alpha)(1-\alpha)^{n-1} \\
    &=(1-\alpha)^{n-1}-\alpha(1-\alpha)^{n-1}
    \label{eq:aux}
  \end{align}
  Inserting \eqref{eq:aux} in \eqref{eq:sum} we get:
  \begin{align}
    S_n&=(1-\alpha)^{n-1}-\alpha(1-\alpha)^{n-1}+\sum_{i=1}^{n} \alpha(1-\alpha)^{n-i}\\
    &=(1-\alpha)^{n-1} + \alpha \sum_{i=1}^{n-1} \alpha(1-\alpha)^{n-1-i}
  \end{align}
  If we call $l=n-1$ we obtain exactly \eqref{eq:sum} but with $l$ instead of 
  $n$. We can repeat this process until the new index happens to be $1$. Then
  we have:
  \begin{equation}
  S_n = (1-\alpha) + \alpha = 1 \qed 
\end{equation}

Lets outline with pseudo-code how this agent would work:

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
\begin{algorithm}
  \caption{}\label{Non-stationary multiarmed-bandit}
  \begin{algorithmic}[1]
  \Procedure{Non-stationary multiarmed-bandit agent}{} \BState count = 0 \BState
  $\textbf{while } \text{count} < N:$ \State action $\gets$ $\argmax Q$ \State
  reward $\gets$ bandit(action) \State Q[action] $\gets$ Q[action] + $\alpha$
  (reward - Q[action]) \State count $\gets$ count + 1 \EndProcedure
  \end{algorithmic}
  \end{algorithm}

  If we pay attention to the pseudo-code, we see that to take the first action
  we need an initial estimate $Q_1$ for each action. These initial estimates
  aren't irrelevant. They play an important role on the convergence of the
  algorithm. For example, if we set $Q_1=0 \; \; \forall a \in \mathcal{A}$, the
  $\argmax Q_1$ will output a random action. Therefore, the training can be
  biased towards that random action depending on the initial reward. Sometimes a
  good strategy is to set high values of $Q_1$ for all actions. This will make
  the agent overoptimistic in the estimate, forcing it to explore all options
  until getting a more realistic estimate of the reward.

  It can be shown that choosing the following learning rate give us an
  exponential recency-weighted average without initial bias:
  \begin{equation}
    \alpha_n = \beta/o_n
  \end{equation}
  where $\beta$ is a constant parameter and $o_n$ is a constant step size
  updating parameter so that:
  \begin{equation}
    o \gets o + \beta(1-o) 
  \end{equation}
  with an initial value of $o_1=0$ for all actions.

\subsection{Modified multi-armed bandit problem}

If we come back to the pendulum example of Chapter \ref{EAmodel}, we can map the
physical set up to a 2-armed bandit problem. We just need to assign the action
of  measuring $L$ to one of the arms and measuring $M$ to the other. Then we set
the function $R$ of the bandit to be $R:\mathcal{A}\times S\rightarrow
\mathbb{R}$ as a composition of Gaussian reward function $r$ with the Analyzer
predicting the period from the outcome. Basically, we assume that the bandit is
the result of the interaction between the Environment, the Analyzer and the
Gaussian comparator (Fig. \ref{fig:Bandit-EA}). \par Note that this multi-armed
bandit problem is quite difficult: it is non-stationary since the Analyzer and
the Environment change each episode and it is non-Markovian since the
performance of the Analyzer depends on the training over all the previous
episodes. However, as we have seen, even very simple agents perform surprisingly
well for this simple case.


\begin{figure}
  \centering 
\includegraphics[scale=0.75]{figures/Bandit-EA.pdf}
\caption{Diagram of how the simplest combination for the E-A architecture
(\ref{fig:simplesetup}) can be modelled as a multi-armed bandit.}
\label{fig:Bandit-EA}
\end{figure}



\par If we can map the simple pendulum example to a multi-armed bandit, it is
evident that many other physical set-ups can be mapped too. So instead of
transforming physical examples to a multi-armed bandit problem, we can design a
customizable multi-armed bandit to represent an infinite variety of physical
situations. However, this bandit is only useful to represent those situations
in which a discrete set of actions is chosen at the beginning of the experiment
without further intervention until the end of the experiment.

In this kind of set up we only have a single experimenter at the beginning of
the learning loop. This means that the state space $\mathcal{X}$  of the
Experimenter's environment is again the empty set $\emptyset$. The action space
$\mathcal{A}$ is finite and discrete with $k \in \mathbb{N}$ possible actions.
The SPE, in the state $s \in S$, after the action $a$ of the Experimenter,
generates an outcome $\textbf{O}(a,s) \in \mathbb{R}^m$, and the value of the
target physical property $\textbf{f}(s') \in \mathbb{R}^l$, where $s' \in S$ is
the state of the SPE after the action $a$. Then, the Analyzer $A$ is asked to
guess $\textbf{f}(s')$ using the value of $\textbf{O}(a,s)$ and potentially the
action $a$ and some additional auxiliary information $I \in  \mathbb{R}^{p}$.
The prediction $\textbf{P} \in \mathbb{R}^l$ of the Analyzer is compared to the
actual value of the physical property $\textbf{f}(s')$ by means of a reward
generating function $r:\mathbb{R}^l\times\mathbb{R}^l\rightarrow\mathbb{R}$. The
output of $r(\textbf{f}(s'),\textbf{P})$ is the output reward of the bandit.

% \par Although for the sake of formality we have introduced many mathematical 
% objects the reader shouldn't be confused with the learning loop, that is 
% rather simple. Let's outline it here in a more informal language:
% \begin{itemize}
%   \item The Experimenter pulls a lever of the bandit.
%   \item A set of $m$ real numbers is generated.
%   \item Some function $\textbf{f}(a,s)$ is computed, unknown to the agent.
%   \item 
% \end{itemize}  

\begin{myex}
  We can illustrate again what each element of the bandit is with the familiar
  pendulum example. In the pendulum example:
  \begin{itemize}
    \item $k=2$, since there are only two possible actions.
    \item $s'=s$ since the actions do not change the state of the pendulum 
    represented by the tuple $(M,L)$.
    \item $f(s')=T(L)=2\pi\sqrt{L/g}$ and therefore $l=1$.
    \item $O(a=0,s=(L,M))=M$ and $O(a=1, s=(L,M))=L$. Therefore $m=1$, since 
    the outcome is just a real number.
    \item $r$ is defined by \eqref{gaussian_reward}
    \item There's no auxiliary information $I$.
  \end{itemize}
  
\end{myex}

Now let us explore different versions of this modified multi-armed bandit.

\subsubsection{Modified multi-armed bandit I}

  In this example, the multi-armed bandit is going to be very simple. There are
  $k$ arms. One of them, the arm $t$ (target) is special. This arm is generally
  selected at random in the first episode. At the beginning of each episode, the
  state of the SPE is generated by associating to each arm a real number $v_k$.
  Each value $v_k$ is sampled from the same uniform distribution
  $\mathcal{U}(v_{min},v_{max})$. Formally, the SPE state space is:
  \begin{align}
    S\equiv\{&s=(v_0, v_1,...,v_k) \in \mathbb{R}^k: \\
    & \forall i \; \; v_i \sim \mathcal{U}(v_{min},v_{max})
    \}
  \end{align}
  Each time a lever $a$ is pulled, the outcome $O(a,s)$ is just the the value
  $v_a$ associated to the arm $a$. We choose the function $f(s)$ to be any real
  function of only the value of the target arm. For instance, $f(v_t)=v_t^2$.
  Then the Analyzer is fed the outcome $O(a,s)=v_a$ to try to predict $f(v_t)$.
  It produces a prediction $A(v_a)=p$ that is used to generate a reward
  $r(p,f(v_t))$ according to \eqref{gaussian_reward}. Then the Analyzer and the
  Experimenters are trained with their respective algorithms.

  \par It is clear that the Analyzer would be able to make a prediction of
  $f(s)$ better than random guessing only if the Experimenter chooses the target
  arm $t$. Note also that the pendulum example is just a particular case of this
  example. We just need to set $t$ as the arm that outputs $L$, and
  $f(v_t)=T(L)$.

  Let's run a few different cases to see how does it perform with different
  configurations.

  \begin{myex}\label{MAB1ex}

    \begin{figure}
      \centering
      \includegraphics[scale=0.5]{figures/Analyzer1.pdf}
      \caption{Two hidden layer feed-forward neural network, with each layer
      consisting in 16 fully connected neurons.}
      \label{Analyzer1}
    \end{figure}


    \begin{figure}[]
      \centering
      \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Actions-MAB1.pdf}
        \caption{Moving average of the actions}
        \label{fig:ActionAverageMAB1}
      \end{subfigure}%
      \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Cum-action-MAB1.pdf}
        \caption{Cumulative number of actions taken by the experimenter}
        \label{fig:actionsMAB1}
      \end{subfigure}
      \caption{Training of example \ref{MAB1ex}. These two figures shows us how
      the agent realizes that in order to get reward it needs to take the target
      action (the lever 6 in this case). In a) we can see the average value of
      the action taken in each episode how it converges to the target lever.
      In (b) we can see the cumulative number of actions taken by the agent and
      how around the $600^\text{th}$ episode the agent finds the target arm and
      chooses it whenever a greedy action is taken. Information: $N=5000$,
      $\beta_\text{Experimenter}=0.01$, $\text{lr}_\text{Analyzer}=0.01$,
      $Q_1=0$, $\sigma=0.05$}
    \end{figure}  



    In this example let us set:

    \begin{itemize}
      \item The number of arms to $k=8$.
      \item The function $f(v_t)$ to be the identity $f(v_t)=v_t$
      \item The Experimenter is based on the algorithm described in
      \ref{SimpleRL}, equipped with an $\epsilon$-greedy exploration phase with
      exponential decrease of the exploration rate. 
      \item The Analyzer is a two hidden layer feed-forward neural network, with
      each layer consisting in 16 fully connected neurons activated with the
      ReLU function (Fig. \ref{Analyzer1} ). The optimizer is just t qhe
      gradient descent algorithm. This Analyzer is of unnecessary complexity for
      this task, but it will be useful later for more complex functions
      $f(v_t)$.
      \item The reward generating function $r$ is defined as in q
      \eqref{gaussian_reward}.
    \end{itemize}

    Running a few tries we find that by setting the number of episodes to
    $N=5000$ the agent always finds the target lever and correctly learns the
    identity function. In the Fig. \ref{fig:actionsMAB1} we can observe how the
    agent discovers the target lever around the episode 600, and once the lever
    is found, as it is expected, the Analyzer converges very quickly to the
    identity function (Fig. \ref{fig:ActionAverageMAB1}). Although this example
    might look trivial, is not evident that it should work. Note that at
    the beginning, neither the Experimenter or the Analyzer has any information
    about the target lever or the function $f(s')$. In the exploration phase,
    the Experimenter chooses random levers and the Analyzer tries to fit the
    function with most of the times useless inputs but $1/8^{\text{th}}$ of the
    times a useful value. Then the Gradient Descent algorithm is applied and
    results to be surprisingly robust to the noise produced by the useless inputs.
    \begin{figure}[]
      \centering
      \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Error-MAB1.pdf}
        \caption{Average error per episode}
        \label{fig:ErrorAverageMAB1}
      \end{subfigure}%
      \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/predictions-MAB1.pdf}
        \caption{Predicted value vs. Target value}
        \label{fig:predictionsMAB1}
      \end{subfigure}
      \caption{Performance of the example \ref{MAB1ex}. These two figures shows
      us how the agent learns to predict the identity function. In a) we can see
      the average value of the error drops quickly to zero as the agent identify
      the correct lever and the exploration rate vanishes. In (b) we can see the
      predictions of the agent after 5000 episodes and how it fits nearly
      perfectly the identity function. Information: $N=5000$,
      $\beta_\text{Experimenter}=0.01$, $\text{lr}_\text{Analyzer}=0.01$,
      $Q_1=0$, $\sigma=0.05$}
    \end{figure} 
  \end{myex}
  \begin{myex} \label{MAB2ex}
    In this example we keep the exact same configuration than in the previous
    example but changing only the function $f(s')$. Instead of 
    the identity we are going to ask for a complicated function and test if the
    agent finds the lever. The chosen function is:
    \begin{equation}
      f(s')=f(v_t)= e^{\text{sin}(v_t)}
    \end{equation}
    Keeping the same hyper-parameters we observe that 5,000 episodes are not
    enough for the agent to succeed. However, trying with 50,000 episodes we
    observe that the agent finds without problems the correct lever (Fig.
    \ref{fig:ActionAverageMAB2}) and successfully learns the function $f(v_t)$
    (Fig. \ref{performanceMBA2}). This reveals that non-trivial correlations can
    proportionate enough bias to initiate the feedback loop, although the
    duration of the exploration phase for it to start increases 
    with the complexity of the correlation.


    \begin{figure}
      \centering
      \includegraphics[scale=0.5]{figures/performanceMBA2 edited.pdf}

      \caption{Graphical representation of the predictions made by the analyzer
      after the training compared to the exact value $e^{\text{sin}(v_t)}$ for 
      the interval $[v_{min}=0,v_{max}=10]$ after 50,000 episodes of training.}
      \label{performanceMBA2}
    \end{figure}


    \begin{figure}[]
      \centering
      \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Actions-MAB2.pdf}
        \caption{Moving average of the actions}
        \label{fig:ActionAverageMAB2}
      \end{subfigure}%
      \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Cum-action-MAB2.pdf}
        \caption{Cumulative number of actions taken by the experimenter}
        \label{fig:actionsMAB2}
      \end{subfigure}
      \caption{Training of example \ref{MAB2ex}. These two figures shows us how
      the agent realizes that in order to get reward it needs to take the target
      action (the lever 6 in this case). In (a) we can see the average value of
      the action taken in each episode and how it converges to the target lever.
      In (b) we can see the cumulative number of actions taken by the agent and
      how around the $2000^\text{th}$ episode the agent finds the target arm and
      chooses it whenever a greedy action is taken. Information: $N=5000$,
      $\beta_\text{Experimenter}=0.01$, $\text{lr}_\text{Analyzer}=0.01$,
      $Q_1=0$, $\sigma=0.05$}
    \end{figure}

  

    \begin{figure}[]
      \centering
      \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Error-MAB2.pdf}
        \caption{Average error per episode}
        \label{fig:ErrorAverageMAB2}
      \end{subfigure}%
      \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/predictions-MAB2.pdf}
        \caption{Predicted value vs. Target value}
        \label{fig:predictionsMAB2}
      \end{subfigure}
      \caption{Performance of the example \ref{MAB2ex}. These two figures shows
      us how the agent learns to predict the identity function. In (a) we can see
      the average value of the error drops to zero as the agent identify
      the correct lever and the exploration rate vanishes. In (b) we can see the
      predictions of the agent after 50,000 episodes and how it fits with
      acceptable accuracy the target function. Information: $N=50,000$,
      $\beta_\text{Experimenter}=0.01$, $\text{lr}_\text{Analyzer}=0.01$,
      $Q_1=0$, $\sigma=0.05$}
    \end{figure} 
  \end{myex}

  \begin{myex} \label{MAB3ex}
    In this example we are going to see what happens if instead of a small
    number of arms, we have a large number of arms in which only one gives the
    relevant value. If we set $k=100$ and a simple but not trivial polynomial
    function $f(v_t)=v_t^2-v_t$ we find that the agent finds the correct lever
    also in less than 50,000 episodes (Fig. \ref{fig:ActionAverageMAB3}). The
    predictions of the analyzer after the training are nearly perfect due to
    the simplicity of the function and the high number of training steps.

    \begin{figure}[]
      \centering
      \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Actions-MAB3.pdf}
        \caption{Moving average of the actions}
        \label{fig:ActionAverageMAB3}
      \end{subfigure}%
      \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Cum-action-MAB3.pdf}
        \caption{Cumulative number of actions taken by the experimenter}
        \label{fig:actionsMAB3}
      \end{subfigure}
      \caption{Training of example \ref{MAB3ex} . These two figures shows us how
      the agent realizes that in order to get reward it needs to take the target
      action (the lever 2 in this case). In a) we can see the average value of
      the action taken in each episode and how it converges to the target lever,
      despite of the large number of levers. In (b) we can see the cumulative
      number of actions taken by the agent and how around the $40,000^\text{th}$
      episode the agent finds the target arm and chooses it whenever a greedy
      action is taken. Information: $N=10^5$, $\beta_\text{Experimenter}=0.01$,
      $\text{lr}_\text{Analyzer}=0.01$, $Q_1=0$, $\sigma=0.05$}
    \end{figure}
  \end{myex}

  These last two simple examples show us that the agent performs well for both,
  complex functions and large action spaces. There is no reason to think that
  larger action spaces or more complex correlations would impose any limitations
  besides an increase on the number of episodes required or the need of a more
  flexible Analyzer. As we will see in another example later, even with a more
  complex configuration and 2,000 different levers the agent is able to find the
  correct lever (although the training took around 5h in an average laptop
  processor).

  \par In the next examples we will try different configurations in which the
  outcome are multidimensional. We will also introduce the concept of auxiliary
  parameters, that would be very useful for more complex configurations of the
  E-A architecture.

  \subsubsection{Modified multi-armed bandit II}
    In this example we are going to set $k=8$ again. In this case, each
    lever is going to have three values associated instead of only one. But 
    we keep only a valid target lever $t$ and the rest output useless
    values. At the beginning of each episode, the
    state of the SPE is generated by associating to each arm three real numbers
     $\{x_a,y_a,z_a\}$. The values ares sampled from the uniform distributions
    $\{\mathcal{U}(v_{min},v_{max})\}_{v=x,y,z}$. Formally, the SPE state space 
    is:
    
    \begin{align}
      S\equiv\{&s=(x_0,y_0,z_0,...,x_k,y_k,z_k) \in \mathbb{R}^{3k}: \\
      & \forall v,i \; \; v_i \sim \mathcal{U}(v_{min},v_{max})
      \}
    \end{align}

    Now we can set $f(s')$ to be 3-variable function, for example:
    \begin{equation}
      f(x_t,y_t,z_t)=x_ty_tz_t
    \end{equation}

    Note that in this case at the input of the Analyzer we will feed three
    parameters instead of one, maintaining the same hidden layers. Keeping the
    rest of the agent settings like in the previous examples we find that it
    discovers the correct action and learns to predict $f(s')$ in less than
    10,000 episodes. This behavior is almost as good as for the single variable
    case (Fig. \ref{fig:ActionAverageMAB4} ). This means that adding more
    elements to the outcomes doesn't affect significantly the performance of the
    algorithm. This was to be expected, since the addition of more parameters
    adds more tools to recognize the correlations at the expense of a maybe more
    complicated computation of the function. However, the used Analyzer has
    enough flexibility to fit to those kind of functions.


    \begin{figure}[]
      \centering
      \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Actions-MAB4.pdf}
        \caption{Moving average of the actions}
        \label{fig:ActionAverageMAB4}
      \end{subfigure}%
      \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Cum-action-MAB4.pdf}
        \caption{Cumulative number of actions taken by the experimenter}
        \label{fig:actionsMAB4}
      \end{subfigure}
      \caption{Training of example for the 3-variable example. These two figures
      shows us how the agent realizes that in order to get reward it needs to
      take the target action (the lever 2 in this case). In (a) we can see the
      average value of the action taken in each episode and how it converges to
      the target lever. In (b) we can see the cumulative number of actions taken
      by the agent and how around the $10,000^\text{th}$ episode the agent finds
      the target arm and chooses it whenever a greedy action is taken.
      Information: $N=10^4$, $\beta_\text{Experimenter}=0.01$,
      $\text{lr}_\text{Analyzer}=0.01$, $Q_1=0$, $\sigma=0.05$}
    \end{figure}

    A question that one may ask is: "What if the agent is allowed to pull more
    than one lever per episode, for example, $n$ levers, to then be asked for a
    prediction that involves the parameters of $l\leq n$ levers?" In such a case
    the problem can be mapped to a single-lever bandit like the ones we have
    seen by associating a lever to each one of the possible combinations of
    pulls. This is, for a bandit with $k$ levers and $n$ pulls each episode, the
    number of possible combinations is just $k\choose n$.  And each new bandit
    would output the $n$ parameters that would result from all the pulls of the
    associated combination. A more interesting, but also more difficult problem,
    is to create an agent that also optimizes in the number of pulls per
    episode, to find the minimum number $l$ of pulls needed to maximize the
    precision of the predictions. This kind of agent would be very interesting
    in the context of minimal representation learning. The main challenge here
    is that changing the number of pulls changes also the number of features
    fed to the Analyzer. A possible solution would be to add a penalization
    to the reward proportional to the number of pulls performed per episode and
    solving the issue of the number of features by using placeholders to keep
    the neural network architecture working. However, the design of this
    architecture is beyond the scope of this work and it is let for future
    investigations.

  \subsubsection{Modified multi-armed bandit III}

  In this version of the multi-armed bandit we are going to introduce a 
  slight change in the architecture of the bandit. This consists in adding
  to the input of the Analyzer some auxiliary information needed to make
  the prediction. In addition to the outcome $\textbf{O}(a,s)$ and potentially
  the action $a$, we can also feed to the Analyzer more parameters that 
  might be relevant to make the prediction. Let us illustrate again this
  concept by using the pendulum example.

  \begin{figure}
    \centering
    \includegraphics{figures/Auxiliary Parameters.pdf}
    \caption{To add the auxiliary parameters to the learning loop we just need
    to make a small modification to the diagram.}
    \label{fig:auxiliaryparameter}
  \end{figure}

  Suppose that, instead of directly asking the Analyzer for the period $T$ of
  the gravity pendulum, what we want the Analyzer to predict is the angle
  $\theta(\tau)$ of the pendulum at a given time $\tau$. The accuracy of the
  Analyzer predicting the angle would determine the reward. However, the
  dynamics are determined by a second order differential equation. In order to
  predict the angle $\theta(\tau)$ correctly, the Analyzer needs at least three
  extra parameters. The time $\tau$ and the values $\theta(0)$ and
  $\dot{\theta}(0)$. It is, in general, a good practice to sample the extra
  features of the Analyzer uniformly at random from a selected interval. This
  would force the Analyzer to learn without bias within the given interval. We
  can see how the learning loop is modified with the new addition (Fig.
  \ref{fig:auxiliaryparameter}). We shouldn't expect any drastic change in the
  performance compared to the multi-parameter case. We are just adding
  parameters, the only difference is that they aren't determined by the
  action of the Experimenter. This means that the Analyzer has some
  degree of information even if the Experimenter takes wrong choices. We can
  expect the Analyzer performing better than random guessing regardless the
  performance of the Experimenter.
  
  \par The results of running this example are summarized in 
  Fig. \ref{completpd}. Like we expected, it worked similarly to previous 
  examples.



  \begin{myex}\label{pendulumcomplete}
    This example will complete the pendulum example. Instead of asking the 
    analyzer to predict the value of the period $T$, we are going to ask for 
    the angle $\theta(\tau)$ of the pendulum. To do it we are also going
    to feed to the Analyzer the instant $\tau$ and the initial values
    $\theta(0)$ and $\dot\theta(0)$. All these values are generated from a
    uniform distribution randomly each episode. We assume that the pendulum
    behaves like an simple harmonic oscillator and therefore:
    \begin{equation}
      f(L,\tau, \theta(0),\dot\theta(0))=\theta(\tau)=
      \theta(0)\text{cos}(\omega\tau)+ \dot\theta(0)\text{sin}(\omega\tau)
    \end{equation}
    where $\omega = \sqrt{g/L}$. Results on Fig. \ref{fig:completepd}.

    \begin{figure}  % spans both columns
      \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\linewidth]{figures/Action-MAB5.pdf}
      \caption{Actions moving average}
      \end{subfigure}
      \hfill % maximize the horizontal distance between the graphs
      \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\linewidth]{figures/Cum-action-MAB5.pdf}
      \caption{Cumulative actions taken}
      \end{subfigure}
      
      \bigskip  % some extra vertical whitespace
      \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\linewidth]{figures/Error-MAB5.pdf}
      \caption{Absolute error moving average}
      \end{subfigure}
      \hfill % maximize the horizontal distance between the graphs
      \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\linewidth]{figures/predictions5.pdf}
      \caption{Perfect vs Predicted}
      \end{subfigure}
      \caption{Summary of the training for the example 
      \ref{pendulumcomplete}. We observe that the actions 
      converge like in previous examples. The Analyzer doesn't 
      completely fit the function, although shows clear signs of 
      learning. This could be due to an inadequate Neural Network
      architecture or insufficient training. Information: $N=40,000$,
      $\beta_\text{Experimenter}=0.01$, $\text{lr}_\text{Analyzer}=0.01$,
      $Q_1=0$, $\sigma=0.05$}
      \label{fig:completepd}
    \end{figure}
  \end{myex}

  \begin{myex} \label{2k}
    In this short example let us see how our agent can solve a multi-armed
    bandit problem with $k=2000$, with an auxiliary parameter and a non-trivial
    function to predict. There is only a target lever $t$ that outputs the value
    $v_t$. Let $u$ be an auxiliary parameter sampled form a uniform distribution
    and fed to the Analyzer. The function asked to predict is: 
    \begin{equation}
      f(u,v_t)=\sqrt{uv_t\text{sin}(v_t)}
    \end{equation}
    We ran it for $N=2\times10^6$ and we found that the agent successfully 
    discovered the correct lever in around $6\times10^5$ episodes
    (Fig. \ref{fig:ActionAverageMAB6}).
    

    \begin{figure}[]
      \centering
      \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Action-MAB6.pdf}
        \caption{Moving average of the actions taken}
        \label{fig:ActionAverageMAB6}
      \end{subfigure}%
      \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/predictions-MBA6.pdf}
        \caption{Predicted value vs. Target value}
        \label{fig:actionsMAB6}
      \end{subfigure}
      \caption{ Summary of the training example \eqref{2k}
         Information: $N=2\times10^6$,
      $\beta_\text{Experimenter}=0.01$, $\text{lr}_\text{Analyzer}=0.01$,
      $Q_1=0$, $\sigma=0.05$}
    \end{figure}
  \end{myex}

  \subsection{Summary of the multi-armed bandit}
  TO BE WRITTEN
% \section{Scenario 2: Spring-Shooting experiment}



\chapter{E-A architecture for sequential experiments}

In the previous section we showed how the simplest version of the
E-A architecture can find the optimal strategy for experiments that can be
expressed as a multi-armed bandit problem. This is: experiments with a discrete
number of action that can be predetermined before the experiment is started.
However, in general it is not possible to express every experiment as a 
multi-armed bandit problem. For example, if the action space is continuous 
(e.g. selecting the position of a detector) or if the experiment 
requires from a dynamical intervention, for example, if in an experiment
consisting of sequential actions, it's needed the result of 
the preceding actions to take the correct choices. In this section we are going
to show how one could tackle this kind of experiments using the E-A 
architecture. 

\section{Sequential experiments}

So far, all the E-A models we have used consists of an Experimenter $E:\emptyset
\rightarrow \mathcal{A}$ whose action $a$ over the environment in the state $s$
produce some outcome $\textbf{O}(a,s)$, and then the outcome and maybe some
additional information are fed to the Analyzer $A:\mathcal{M} \rightarrow
\mathcal{P}$ to output a prediction about a physical property of the system.
Then such property is observed (or in our case, simulated) and compared to the
prediction to generate a reward and a loss, to train both $E$ and $A$. 

This simple structure is enough for experiments in which we can group all
actions and measurements to be taken in a single action at the beginning of the
experiment. For example, if the experiment consists on measuring different
parameters of an electric circuit to know the power consumed by a section of the
circuit. We can group all those choices of measurements, e.g. measuring the
intensity at node A and the voltage at node B, in a single action taken at the
beginning of an episode. This is because the outcomes of the measurements don't
condition each other. However, in other experiments the outcomes of previous
actions may condition the actions to be taken in order to get the correct
parameters for a prediction. We will call this experiments \textit{sequential}, 
since a number of actions need to be taken sequentially.
Let us illustrate the concept with an example of a sequential experiment:

\begin{myex} \label{scalex}
  
  Imagine an experiment that has the objective of determining the mass $M$ of an
  object with the following tools:
  
  \begin{itemize}
    \item A beam balance with two plates that leans to the side of the heavier
    plate. 
    \item A set of calibrated weights consisting of:
    \begin{itemize}
      \item A mass of 100 g
      \item Two masses 200 g
      \item A mass of 500 g
      \item A mass of 1 Kg
    \end{itemize}  
  \end{itemize}

If the mass $M$ to be weighted is less than $2$Kg we can find value of the mass
$M$, with a uncertainty of 100 g, in only five uses of the scale. For example,
if the mass is 1.3 Kg a procedure would be the following:
\\\\
\begin{minipage}[]{0.5\linewidth}
  \centering
  \strut\vspace*{-\baselineskip}\newline\includegraphics[width=0.8\linewidth]{figures/scale.pdf}
\end{minipage}
\begin{minipage}[]{0.4\linewidth}
  \begin{enumerate}
  \item M vs 2 Kg $\rightarrow$ right
  \item M vs 1 Kg $\rightarrow$ left
  \item  M vs 1.5Kg $\rightarrow$ right
  \item  M vs 1.2 Kg $\rightarrow$ left
  \item  M vs 1.4 kg $\rightarrow$ right
  \item  We know that the mass is: $1.3$ Kg $\pm 100$ g
\end{enumerate}
\end{minipage}
\\\\

However, in order to find the value of $M$ in five uses of the scale or less we
can't have a prefixed strategy at the start of the experiment. We need to
observe the outcomes of each measurement individually and respond accordingly.
We can not express then this experiment as a multi-armed bandit decision
problem and therefore is sequential.

\end{myex}

\section{E-A for sequential experiments}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figures/sequential-EA.pdf}
  \caption{Diagram of the learning loop for a E-A architecture for a 
  sequential experiment with $D$ steps. The structure is the same as for 
  the simple case but concatenating $D$ Experimenters instead of a single one.}
  \label{fig:SequentialEA}
\end{figure}

In this section we are going to explain how to use the E-A architecture to
tackle sequential experiments. The basic idea behind this approach is to
concatenate as many Experimenters as sequential actions we need to take and each
Experimenter is fed with the information gathered by the actions and outcomes
gathered by the preceding Experimenters. 

Suppose we have a sequential experiment that needs from $D$ sequential actions.
Then we would have $D$ different Experimenters $E_n:\mathcal{X}_n\rightarrow
\mathcal{A}_n$ with $\mathcal{X}_0\equiv\emptyset$. Each $\mathcal{A}_n$ can be
different depending on the nature of the experiment, since the Experimenters
$E_n$ could be different reinforcement learning agents with different
architectures. The Experimenters' environments $\mathcal{X}_n$ can consist of
any information stored in the buffer at the moment they are fed to $E_n$.The SPE
is in the state $s_n \in S$ when the action $a_n \in \mathcal{A}_n$ is taken and
it produces the outcome $\textbf{O}_n(a_n, s_n) \in \mathbb{R}^{d_n}$ where
$d_n$ is the number of parameters that are outputted after the action $a_n$.
Then, as in the single Experimenter version, all the information in the buffer
together with auxiliary parameters can be stored into $m\in\mathcal{M}$ to fed
the Analyzer $A:\mathcal{M}\rightarrow \mathcal{P}$. Then the prediction is
compared with the observed/simulated value of the property to be predicted
with a reward generation function $r_n$. The reward could be different for each
experimenter depending on the nature of the architecture. However, in this work
only explored the architecture with a single reward generation function $r$ for
all Experimenters.

One of the main limitations of this approach is that we need to fix the number
$D$ of actions beforehand. In some experiments this may not be relevant since
the number $D$ is fixed by the experiment itself. However, in others this fact
might be important. For example, in the context of the example \ref{scalex}, we
may ignore what is the minimum number of actions needed to determine the mass
with an uncertainty of less than 100 g. The agent will only search for the best
strategy for the of actions $D$ given, which may not be optimal.


\section{Example}

In this section we are going to show an example of a sequential experiment
in which the A-E architecture finds the optimal strategy.

\subsection{The experiment}

We have a box of mass $M$ on an infinite horizontal surface with friction
coefficient $\mu$ respect to the mass $M$. The SPE is programmed so that a canon
shoots a bullet of mass $m$ with velocity $v$ that collides inelastically with
$M$ when an episode is started. The mass $M$ is the only parameter restarted
each episode by sampling uniformly at random from the interval $[M_{min},
M_{max}]$. We can calculate the distance traveled by the box using the
conservation of momentum. The moment of the bullet is $p_b=mv$. Applying the
conservation of momentum we can obtain the velocity of the box after the impact:
\begin{equation}
  v_M = \frac{p_M}{M}=\frac{m}{M}v
\end{equation}
We know that the friction force is $F=\mu N=\mu mg$. When the box stops, due to
the conservation of energy, the work realized by the friction force must be the
kinetic energy $K$ of the mass $M+m$ of the box after the impact of the bullet.
Since the friction force is assumed to be constant, the work done is $W=Fx=\mu
mgx$, where $x$ is the distance traveled until it stops. Equating this term to
the kinetic energy we obtain:
\begin{equation}
  \label{displacement} 
  x=\frac{1}{2g\mu}\left(\frac{vm}{M+m} \right)^2
\end{equation}

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.5]{figures/Experiment2.pdf}
  \caption{Diagram explaining the physical environment. The canon shoots a 
  bullet of mass $m$ with velocity $v$. When it collides inelastically with the 
  box of mass $M$, the box displaces. Due to the friction force it stops in
  one of the four zones $A, B, C$ or $D$.}
  \label{fig:Box}
\end{figure}

We give the agent access to two sensors to measure the position of the box:
\begin{itemize}
\item \textbf{A low precision but wide range (LPWR) position  sensor}: it works
as follow: the landing zone is divided into four regions or zones of equal size.
This sensor outcomes the middle point of the zone in which the box has landed.
For example, in the situation depicted in Fig. \ref{fig:Box} this sensor would
produce the outcome: $3$, since it is the middle point of the zone $B$. This
allows to know roughly where the box landed, but with low accuracy. 
\item \textbf{A high precision but narrow range (HPNR) position sensor}: It
works as follow: to activate it you need also to specify a region and if the box
landed on the specified region the outcome of the sensor is the exact position
of the box. Otherwise it outputs a constant negative value (set to $-1$)
representing a failed measurement. For example, in the situation depicted in
Fig. \ref{fig:Box} this sensor is placed in the zone $B$ it will output the
value of $x$. If placed in any other zone it outputs $-1$.
\end{itemize}

The goal of the agent is to predict the position in which the box will
land for a shooting with velocity $v_{test}$ and bullet mass $m$, but only with
$\textbf{two uses of the sensors}$. The velocity $v_{test}$ is selected randomly
each episode from a uniform distribution for the interval $(v-\frac{v}{2},
v+\frac{v}{2})$. This velocity $v_{test}$ would be the auxiliary parameter fed
to the Analyzer. 

Using the same procedure that we did for the first shooting, we can find that
the new displacement $d$ of the box after the shooting is:
\begin{equation}
  d=\frac{1}{2g\mu}\left(\frac{v_{test}m}{M+2m} \right)^2
\end{equation}

Since $d$ depends on the mass $M$, and $M$ can be inferred from the first
displacement $x$ using \eqref{displacement}, the agent has enough information
to predict the displacement of the test shooting if it detects correctly the 
position $x$ of the box.

This experiment is sequential because to always obtain the exact position of the
box, the agent needs to use sequentially the sensors in a specific way. This
strategy consists on choosing in the first try the LPWR sensor, to detect in
which zone the box landed, and in the second use of the sensors to choose the
HPNR sensor and place it in the zone inferred from the first outcome.

\subsection{Mathematical characterization of the experiment}

Let us contextualize each element of the training using the mathematical
formulation used in the text.

\begin{itemize}
  \item The state of the SPE is determined by the tuple $s=(M,v_{test})$.
  \item The E-A agent has two experimenters: $E_1:\mathcal{X}_1
  \rightarrow\mathcal{A}$ and $E_2:\mathcal{X}_2 \rightarrow\mathcal{A}$ where
  $\mathcal{X}_1=\emptyset$ and $\mathcal{X}_2=\mathcal{A}\times \mathbb{R}$,
  since we feed the second experimenter the action $a_1$ of $E_1$ and the
  outcome $O(a_1)$ of its action.
  \item The action space for both experimenters is $\mathcal{A}=\{0,1,2,3,4\}$, 
  with the action $0$ representing the action of using the LPWR sensor, and
  the actions $\{1,2,3,4\}$ represent the actions of using the HPNR and placing 
  it in the zones $A, B, C$ and $D$ respectively.
  \item The analyzer is $A:\mathcal{M}\rightarrow \mathbb{R}$, where each element
  of $\mathcal{M}$ is a tuple \\
  $\left(a_1,O(a_1,s),a_2,O(a_2),v_{test}\right)$.
  \item The quantity to be predicted is
  $f(s)=d=\frac{1}{2g\mu}\left(\frac{v_{test}m}{M+2m} \right)^2$
  \item The reward function is 
  $r(\Delta)=\exp\left(-\frac{1}{2}\left( \frac{\Delta}{\sigma} \right)
  ^2\right)$, where $\Delta = \frac{f(s)-p}{p}$ and $p$ is the output of the
  analyzer.
  \item The loss function for the Analyzer is $L(f(s),p)=(f(s)-p)^2$
\end{itemize}

\subsection{Technical details of the agents}
\subsubsection{Experimenter 1}
$E_1$ is the same kind of experimenter that we used in the previous examples
(\ref{Non-stationary multiarmed-bandit}), since for $E_1$ everything looks like
a multi-armed bandit. The agent was subjected to an $\epsilon$-greedy 
exploration phase with linear decrease of the exploration rate.
\subsubsection{Experimenter 2}
For the Experimenter 2 now we have inputs, so the reinforcement learning
algorithm of $E_1$ is not valid. Since some of the inputs are real numbers, we
opted to use a standard Deep Q Network algorithm with online training (See
Appendix [TODO]). We also tried a modified version of the Deep Q Network (DQN)
algorithm equipped with a memory and two neural networks (see appendix) to
improve the performance. We could have opted for a normal Q-learning algorithm
as well, but would have required some amount of hardcoding, which is undesirable
in our purpose of finding agents as independent as possible. 
\\
\noindent The structure of the neural network for this agent is a two hidden layers  
(16x16) fully connected sigmoidal neurons for both versions. We ignore if this 
is an appropriate structure since we haven't performed an optimization of the
hyperparameters.

\noindent The agent was subjected parelly to the same $\epsilon$-greedy
exploration phase than $E_1$.

\subsubsection{Analyzer}

The Analyzer in this case is a sequential feedforward neural network with 
3 hidden layers (20x10x2) fully connected ReLU neurons.

\subsection{Results}



%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%






\begin{appendix}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% HEADER APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%






\lhead[\fancyplain{\scshape Appendix \thechapter}
{\scshape Appendix \thechapter}]
{\fancyplain{\scshape \leftmark}
  {\rightmark}}
%
\rhead[\fancyplain{\scshape \leftmark}
{\scshape \leftmark}]
{\fancyplain{\scshape Appendix \thechapter}
  {\scshape Appendix \thechapter}}
%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTERS APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\include{f}






\chapter{Appendix ?} \label{app:pgm}

\blindtext







%
%
%\include{derivations}
% \chapter{Another Chapter in the Appendix}
% \label{cha:anotherchapterintheappendix}
% 
% Each chapter of the appendix is better included by the command 
% \emph{$\backslash$include\{file\}}.
% %



\clearpage
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% HEADER BIBLIOGRAPHY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\lhead[\fancyplain{\scshape Appendix}
{\scshape Appendix}]
{\fancyplain{\scshape \leftmark}
  {\scshape \leftmark}}
%
\rhead[\fancyplain{\scshape \leftmark}
{\scshape \leftmark}]
{\fancyplain{\scshape Appendix}
  {\scshape Appendix}}
  
  
  
  
  
  
  
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% LIST OF FIGURES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \addcontentsline{toc}{chapter}{\numberline{}List of Figures}
% \listoffigures
%\clearpage
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% LIST OF TABLES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \addcontentsline{toc}{chapter}{\numberline{}List of Tables}
% \listoftables
%\clearpage
%


%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BIBLIOGRAPHY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{plain}
\bibliography{references}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% END OF APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\end{appendix}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% INDEX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\printindex
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% END FRAME
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\end{document}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
