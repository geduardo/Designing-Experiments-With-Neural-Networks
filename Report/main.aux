\relax 
\providecommand\hyper@newdestlabel[2]{}
\@nameuse{bbl@beforestart}
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{USenglish}{}
\babel@aux{USenglish}{}
\babel@aux{USenglish}{}
\citation{owidlifeexpectancy}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Preface}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{sinatra2015century,BornmannRudiger}
\citation{publications7010018}
\citation{Alkhateeb}
\citation{Saltelli,begley2012raise}
\citation{Sabine}
\citation{Fanelli2628}
\citation{Carleo_2019}
\citation{Melnikov_2018}
\citation{iten2020discovering}
\citation{nautrup2020operationally}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Minimal model for science}{5}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{science}{{2}{5}{Minimal model for science}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{5}{section.2.1}\protected@file@percent }
\newlabel{sec:intro_science}{{2.1}{5}{Introduction}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}First assumption: Dynamicality}{5}{subsection.2.1.1}\protected@file@percent }
\newlabel{example1}{{2.1.1}{6}{}{mythm.2.1.1}{}}
\newlabel{DynamicalLaw1}{{2.1}{6}{}{equation.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Graphical representation of the dynamical change of a universe with $n=8$.\relax }}{7}{figure.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}The scientific method}{7}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Second assumption: Emergency}{8}{subsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Machine Learning theory}{9}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experimenter-Analyzer model}{10}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Diagram of the simplest combination of elements. The feedback loop consists on the following: 1. The Experimenter takes an action. 2. The environment gives back an outcome resulting from the action. 3. The action is taken by the analyzer and used to make a prediction. 4. The prediction about the environment and the real value of the environment are compared to generate the loss and the reward to train the Analyzer and the Experimenter.\relax }}{11}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:simplesetup}{{4.1}{11}{Diagram of the simplest combination of elements. The feedback loop consists on the following: 1. The Experimenter takes an action. 2. The environment gives back an outcome resulting from the action. 3. The action is taken by the analyzer and used to make a prediction. 4. The prediction about the environment and the real value of the environment are compared to generate the loss and the reward to train the Analyzer and the Experimenter.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Diagram of a simple learning loop with buffer. In this case, the process is identical to the process depicted in Fig. \ref  {fig:simplesetup} with the difference that the values of the action and the outcome are stored in the buffer, and then passed to the analyzer. In this case the information about which action was taken is available to the Analyzer.\relax }}{13}{figure.caption.5}\protected@file@percent }
\newlabel{fig:simplesetupbuffer}{{4.2}{13}{Diagram of a simple learning loop with buffer. In this case, the process is identical to the process depicted in Fig. \ref {fig:simplesetup} with the difference that the values of the action and the outcome are stored in the buffer, and then passed to the analyzer. In this case the information about which action was taken is available to the Analyzer.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}General elements and definitions}{14}{section.4.1}\protected@file@percent }
\newlabel{update_rule1}{{4.2}{15}{}{equation.4.2}{}}
\newlabel{argmax_policy}{{4.3}{15}{}{equation.4.3}{}}
\citation{cybenko1989approximation}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Simple learning loop\relax }}{17}{algorithm.1}\protected@file@percent }
\newlabel{pendulum}{{1}{17}{Simple learning loop\relax }{algorithm.1}{}}
\newlabel{fig:ActionAveragePendulum}{{4.3a}{18}{Moving average of the actions\relax }{figure.caption.6}{}}
\newlabel{sub@fig:ActionAveragePendulum}{{a}{18}{Moving average of the actions\relax }{figure.caption.6}{}}
\newlabel{fig:RewardAveragePendulum}{{4.3b}{18}{Moving average of the reward\relax }{figure.caption.6}{}}
\newlabel{sub@fig:RewardAveragePendulum}{{b}{18}{Moving average of the reward\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces (a) The moving average of the actions taken in each episode. We can see how it starts taking both actions evenly with an average of 0.5. This is expected since at the beginning $\epsilon \approx 1$. As $\epsilon $ decreases the Experimenter starts to take more greedy actions and gets biased towards the action 1 (measure $L$). This bias appears as a result of the existing correlation between $L$ and $T$, but not between $M$ and $T$. This correlation is exploited by the SGD algorithm for the neural network of the Analyzer. (b) The moving average of the reward obtained in each episode. At the beginning the reward obtained is low and similar to those obtained with random guesses. However, it increases slowly, apparently because the Analyzer starts to exploit slightly the correlation between $L$ and $T$. This slight improvement over the prediction for the outcomes of Action 1, creates the bias that starts the feedback loop. Values: $\sigma =0.05$, $N=10000$, $\text  {lr}_\text  {Analyzer}=0.01$, Optimizer = Adam\relax }}{18}{figure.caption.6}\protected@file@percent }
\newlabel{fig:PendulumTraining}{{4.3}{18}{(a) The moving average of the actions taken in each episode. We can see how it starts taking both actions evenly with an average of 0.5. This is expected since at the beginning $\epsilon \approx 1$. As $\epsilon $ decreases the Experimenter starts to take more greedy actions and gets biased towards the action 1 (measure $L$). This bias appears as a result of the existing correlation between $L$ and $T$, but not between $M$ and $T$. This correlation is exploited by the SGD algorithm for the neural network of the Analyzer. (b) The moving average of the reward obtained in each episode. At the beginning the reward obtained is low and similar to those obtained with random guesses. However, it increases slowly, apparently because the Analyzer starts to exploit slightly the correlation between $L$ and $T$. This slight improvement over the prediction for the outcomes of Action 1, creates the bias that starts the feedback loop. Values: $\sigma =0.05$, $N=10000$, $\text {lr}_\text {Analyzer}=0.01$, Optimizer = Adam\relax }{figure.caption.6}{}}
\newlabel{fig:RealvsPredictedPendulum}{{4.4a}{18}{Real T vs Predicted T\relax }{figure.caption.7}{}}
\newlabel{sub@fig:RealvsPredictedPendulum}{{a}{18}{Real T vs Predicted T\relax }{figure.caption.7}{}}
\newlabel{fig:ErrorAveragePendulum}{{4.4b}{18}{Moving average of the error\relax }{figure.caption.7}{}}
\newlabel{sub@fig:ErrorAveragePendulum}{{b}{18}{Moving average of the error\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces (a) A sample of the predictions made by trained agent. We can see how it fits the $y=x$ line, showing that it learned the relation between $L$ an $T$. (b) In this figure we can see how the error decreases to almost zero with very low deviation.Values: $\sigma =0.05$, $N=10000$, $\text  {lr}_\text  {Analyzer}=0.01$, Optimizer = Adam \relax }}{18}{figure.caption.7}\protected@file@percent }
\newlabel{fig:PendulumError}{{4.4}{18}{(a) A sample of the predictions made by trained agent. We can see how it fits the $y=x$ line, showing that it learned the relation between $L$ an $T$. (b) In this figure we can see how the error decreases to almost zero with very low deviation.Values: $\sigma =0.05$, $N=10000$, $\text {lr}_\text {Analyzer}=0.01$, Optimizer = Adam \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}E-A architecture in different scenarios}{19}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Scenarios}{{5}{19}{E-A architecture in different scenarios}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Scenario 1: Modified multi-armed bandit}{19}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}General multi-armed bandit problem}{19}{subsection.5.1.1}\protected@file@percent }
\citation{sutton2018reinforcement}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Simple Reinforcement Learning algorithm for nonstationary multi-armed bandit}{20}{subsection.5.1.2}\protected@file@percent }
\newlabel{simpleQ}{{5.2}{20}{Simple Reinforcement Learning algorithm for nonstationary multi-armed bandit}{equation.5.2}{}}
\newlabel{weighted_sum}{{5.7}{21}{Simple Reinforcement Learning algorithm for nonstationary multi-armed bandit}{equation.5.7}{}}
\newlabel{eq:sum}{{5.8}{21}{Simple Reinforcement Learning algorithm for nonstationary multi-armed bandit}{equation.5.8}{}}
\newlabel{eq:aux}{{5.10}{21}{Simple Reinforcement Learning algorithm for nonstationary multi-armed bandit}{equation.5.10}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces \relax }}{22}{algorithm.2}\protected@file@percent }
\newlabel{Non-stationary multiarmed-bandit}{{2}{22}{\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Scenario 2: Spring-Shooting experiment}{22}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Scenario 3: Shooting with two position sensors}{22}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix ?}{23}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:pgm}{{A}{23}{Appendix ?}{appendix.A}{}}
\bibstyle{plain}
\bibdata{references}
\bibcite{Alkhateeb}{{1}{}{{}}{{}}}
\bibcite{begley2012raise}{{2}{}{{}}{{}}}
\bibcite{BornmannRudiger}{{3}{}{{}}{{}}}
\bibcite{Carleo_2019}{{4}{}{{}}{{}}}
\bibcite{cybenko1989approximation}{{5}{}{{}}{{}}}
\bibcite{Fanelli2628}{{6}{}{{}}{{}}}
\bibcite{Sabine}{{7}{}{{}}{{}}}
\bibcite{iten2020discovering}{{8}{}{{}}{{}}}
\bibcite{owidlifeexpectancy}{{9}{}{{}}{{}}}
\bibcite{Melnikov_2018}{{10}{}{{}}{{}}}
\bibcite{nautrup2020operationally}{{11}{}{{}}{{}}}
\bibcite{Saltelli}{{12}{}{{}}{{}}}
\bibcite{sinatra2015century}{{13}{}{{}}{{}}}
\bibcite{sutton2018reinforcement}{{14}{}{{}}{{}}}
\bibcite{publications7010018}{{15}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
