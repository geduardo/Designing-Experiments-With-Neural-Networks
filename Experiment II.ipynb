{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "'Python Interactive'",
      "language": "python",
      "name": "184cdf6e-8a67-43e9-8238-48f1530b039f"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "333.212px",
        "left": "1540.32px",
        "right": "20px",
        "top": "64px",
        "width": "561.771px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Experiment II.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgEBUJGZsdXu",
        "colab_type": "text"
      },
      "source": [
        "# Experiment II\n",
        "\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook contains the code to execute the second experiment of this project. The basic set up consists on three elements:\n",
        "\n",
        "- An **environment**: A mass $M$ and a infinite horizontal surface with friction coefficient $\\mu$ respect to the mass $M$. This environment automatically shoots a bullet of mass $m$ with a velocity $v$ when an episode is started. The mass $M$ is the only parameter restarted each episode by sampling uniformly at random from the interval $[M_\\text{min}, M_\\text{max}]$. For more information and details, go to the [class documentation](TODO) of this environment. \n",
        "- An **agent** that consists of two sub-agents:\n",
        "    - **Experimenter** or **policy maker:** its objective is to decide which actions to take to gather the data. After the automatic shooting we give the agent the choice between two sensors and two tries per episode.\n",
        "        - A low precision but wide range (LPWR) position  sensor. It works as follow: the landing zone is divided into four regions or zones of equal size. Each time the box is shot it lands in one of the zones and this sensor if activated gives the middle point of the zone. For example, in the image below this sensor would produce the outcome: $3$, since it's the middle point of the zone $B$. This allows to know roughly where the box landed, but with low accuracy.\n",
        "        - A high precision but narrow range (HPNR) position sensor. It works as follow: to activate it you need to specify a region and if the box landed on the specified region the outcome of the sensor is the exact position of the box, otherwise it outputs a constant negative value (set to $-1$) representing a failed measurement. For example, in the image below if this sensor is placed in the zone $B$ it will output the value of $x$ and if placed in any of the other zones it outputs $-1.\n",
        "        \n",
        "    - **Analyzer** or **encoder:** the goal of this agent is to process the data collected by the Experimenter and to use it to make predictions about physical properties of the system. In this case the Analyzer takes the actions and the outcomes of the experimenter to make a prediction about the position of the mass *M* after a test shooting. In this test shooting the velocity of the bullet, known to the analyzer, is sampled uniformly at random from a limited range. \n",
        "    \n",
        "- **Orchestration**: we call orchestration everything that connects the environments and the agents to build the training loop. The code in this notebook is the orchestration. \n",
        "\n",
        "Note that this set up is slightly different from an usual Reinforcement Learning set up, because the rewards do not come from the environment but from the performance of the agent to predict aspects of the environment. Another perspective to this set up would be to include the Analyzer as another element of the environment. However, assuming that perspective means that we have a environment that changes the reward system dynamically, which can be quite a challenging Reinforcement Learning problem. \n",
        "\n",
        "The goal of the Analyzer is to understand the data provided by the measurements of the Experimenter and unveil the correlation between those measurements and the distance traveled by the mass $M$ in the test shooting. The goal of the Experimenter is to learn the optimal strategy to use the sensors to provide the best possible data to the Analyzer. In this case the strategy is obvious: first to take the LPWR sensor to find in which zone the mass landed, and then to use that information to place the HPNR sensor in that zone. The main difficulty here is that the Experimenter needs to memorize the outcome of the first measurement and use it to choose the second measurement. It requires a dynamical strategy for the experiment instead of a predefined one from the beginning.  \n",
        "\n",
        "\n",
        "![alt text](https://github.com/geduardo/Designing-Experiments-With-Neural-Networks/blob/master/Experiment2.svg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "868hSAN2sdXw",
        "colab_type": "text"
      },
      "source": [
        "## Orchestration\n",
        "\n",
        "In this section we are going to implement the training. The code is explained with in-line comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "EFZWwXKUsdXw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "0fe05705-d18a-4bb9-fec1-608a64a54be5"
      },
      "source": [
        "# IMPORTS ===================================================================\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "from keras.utils import to_categorical\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "# We import the agents and the environment from the \n",
        "# modules created in advance.\n",
        "from Modules.rlagents import Q_Learning, DQN\n",
        "from Modules.analyzers import Two_Layers_single_output\n",
        "from Modules.environments import Two_Sensors\n",
        "\n",
        "# CONSTANTS, INSTANTIATIONS, INITIALIZATION ===============================\n",
        "\n",
        "N_Episodes = 2000000 # Number of episodes to train \n",
        "count = 0 # Initialization of a counter\n",
        "\n",
        "# We instantiate the classes of each element\n",
        "env = Two_Sensors() \n",
        "experimenter1 = Q_Learning(iterations=N_Episodes, output_size=5)\n",
        "experimenter2 = DQN(iterations=N_Episodes, input_size=6, output_size=5)\n",
        "analyzer = Two_Layers_single_output(input_size=5)\n",
        "\n",
        "# Save time for verbose purposes\n",
        "t0 = time.time()\n",
        "t1 = time.time()\n",
        "\n",
        "# First random initialization of the variables. This part is needed since \n",
        "# some recurrent parts of the loop need from some data to start with.\n",
        "\n",
        "action1 = experimenter1.get_next_action()\n",
        "outcome1 = env.take_action(action1)\n",
        "action2 = random.randint(0,4)\n",
        "outcome2 = env.position_exact\n",
        "d_test = random.uniform(0.8,8)\n",
        "y_predicted = [0]\n",
        "reward = 0\n",
        "\n",
        "# Auxiliar variables for collecting data\n",
        "values = []\n",
        "df=pd.DataFrame(columns = ['Zone', 'Exact position', 'Action 1', \n",
        "                           'Outcome 1', 'Action 2', 'Outcome 2', 'Reward', \n",
        "                           'd_predicted', 'd_test', 'Exploration rate'])\n",
        "total_reward_list = []\n",
        "\n",
        "# MAIN LOOP =================================================================\n",
        "\n",
        "while count < N_Episodes:\n",
        "    \n",
        "    # Record the data from the previous episode.\n",
        "    if env.position_zone==1:\n",
        "        zone='A'\n",
        "    elif env.position_zone==3:\n",
        "        zone='B'\n",
        "    elif env.position_zone==5:\n",
        "        zone='C'\n",
        "    elif env.position_zone==7:\n",
        "        zone='D'\n",
        "    values.append([zone, env.position_exact, \n",
        "                   action1, outcome1, action2, outcome2, float(reward), \n",
        "                   y_predicted[0], d_test, experimenter2.exploration_rate ])\n",
        "    total_reward_list.append(env.total_reward)\n",
        "   \n",
        "    # First part of the loop ...-...........................................\n",
        "    \n",
        "    # Store the ending state of the previous episode\n",
        "    old_state =  np.append(to_categorical(action1, 5), [outcome1])\n",
        "    # Restart the mass of the environment\n",
        "    env.restart_mass()\n",
        "    # Get the new action of experimenter 1 and its resulting outcome\n",
        "    action1 = experimenter1.get_next_action()\n",
        "    outcome1 = env.take_action(action1)\n",
        "    # Store the new state in a variable\n",
        "    current_state = np.append(to_categorical(action1, 5), [outcome1])\n",
        "    # We use the old_state and the current_state to train the experimenter 2.\n",
        "    # We can understand this as the experimenter2 being a traditional RL \n",
        "    # agent interacting with an environment whose states consists on the\n",
        "    # action and outcome of the experimenter 1.\n",
        "    experimenter2.online_train(old_state, action2, reward, current_state)\n",
        "    # Let the experimenter 2 take a new action and store its outcome\n",
        "    action2 = experimenter2.get_next_action(state=current_state)\n",
        "    outcome2 = env.take_action(action2)\n",
        "    # Do a test shooting to train the analyzer\n",
        "    v_train, d_train = env.test_shooting()\n",
        "    measurements = env.get_measurements(action1, outcome1, action2, outcome2,\n",
        "                                         v_train)\n",
        "    X_train, y_train = env.reshape_for_analyzer(measurements, d_train) \n",
        "    analyzer.train(X_train, y_train)\n",
        "    \n",
        "    # Do another test shooting to test the performance of the analyer and\n",
        "    # generate the reward.\n",
        "    v_test, d_test = env.test_shooting()\n",
        "    measurements = env.get_measurements(action1,outcome1, action2, outcome2,\n",
        "                                         v_test)\n",
        "    X_test, y_test = env.reshape_for_analyzer(measurements, d_test)\n",
        "    \n",
        "    # Predict value for the position of the mass \"d\" and generate reward.\n",
        "    y_predicted = analyzer.predict(X_test)[0]\n",
        "    reward = env.give_reward(y_predicted, d_test)\n",
        "    \n",
        "    # Use the reward to train the first experimenter.\n",
        "    experimenter1.update(action1, reward)\n",
        "\n",
        "# Display training status....................................................\n",
        "    count = count + 1\n",
        "    refresh_rate = 5000\n",
        "    if count % (N_Episodes/refresh_rate) == 0:\n",
        "        clear_output()\n",
        "        t2 = time.time()\n",
        "        m, s = divmod(t2-t1, 60)\n",
        "        mt, st = divmod(t2-t0, 60)\n",
        "        me, se = divmod(((t2-t0)/count)*N_Episodes, 60)\n",
        "        mr, sr = divmod(((t2-t0)/count)*N_Episodes-t2+t0, 60)\n",
        "        print (str(int(count)) + '/' +  str(N_Episodes) + \" episodes\" + \n",
        "               '(' + str(100*count/N_Episodes)+ '%)')\n",
        "        print('Elapsed time: {} min {}s'.format(int(mt), int(st)))\n",
        "        print(\"\"\"Est. completion time: {} min {}s,\n",
        "        Est. remaining time: {} min {}s\"\"\".format(int(me), int(se), int(mr),\n",
        "                                                  int(sr)))\n",
        "        t1 = time.time()\n",
        "print('Training completed!')\n",
        "\n",
        "\n",
        "plt.plot(range(len(total_reward_list)), total_reward_list)\n",
        "plt.ylabel(\"Cumulative reward\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "df=pd.DataFrame(values, columns = ['Zone', 'Exact position', 'Action 1', \n",
        "                                   'Outcome 1', 'Action 2', 'Outcome 2', \n",
        "                                   'Reward', 'd_predicted', 'd_test', \n",
        "                                   'Exploration rate'])\n",
        "df.to_csv('df_nuevo.csv')\n",
        "df.round(2)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7a97c7c0a592>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# We import the agents and the environment from the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# modules created in advance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mModules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrlagents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQ_Learning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mModules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTwo_Layers_single_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mModules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTwo_Sensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Modules'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gryNWUcusdX1",
        "colab_type": "text"
      },
      "source": [
        "## Visualization of the training and discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnWtJ7kYsdX1",
        "colab_type": "text"
      },
      "source": [
        "In this section we are going to plot and discuss several data of the training process. First we are going to see how the model works. To do it let's trim our dataset to include only a sample from the very last part of the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ7Qga_XsdX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zones = ['A', 'B', 'C', 'D']\n",
        "sample_size = 0.1\n",
        "sample=df.loc[np.r_[N_Episodes-N_Episodes*sample_size:N_Episodes], :]\n",
        "# We plot an histogram of the zone distribution to observe the expected\n",
        "# unbalanced distribution of landing zones\n",
        "for zone in zones:\n",
        "    sample['Zone'].hist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA78rlqdsdX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "figs, axs = plt.subplots(2,2)\n",
        "j=0\n",
        "k=0\n",
        "for zone in zones:\n",
        "    df_aux = sample[sample['Zone']==zone]\n",
        "    aux = range(0, int(np.ceil(max(df_aux['d_test']))))\n",
        "    axs[k,j].plot(df_aux['d_test'], df_aux['d_predicted'],'.',label=\"Predictions\")\n",
        "    axs[k,j].plot(aux, aux,'--',label = 'y=x')\n",
        "    axs[k,j].set_title(zone)\n",
        "    axs[j,k].set(xlabel='d test', ylabel='d predicted')\n",
        "    axs[j,k].label_outer()\n",
        "    j=(j+1)%2\n",
        "    if j==0:\n",
        "        k=(k+1)%2\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.label_outer()\n",
        "plt.savefig('generated_figures/AllZones.svg')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0gdsi0EusdX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_pred_vs_test(sample,zone):\n",
        "    df_aux = sample[sample['Zone']==zone]\n",
        "    plt.plot(df_aux['d_test'], df_aux['d_predicted'],'.',label=\"Predictions\")\n",
        "    aux = range(0, int(np.ceil(max(df_aux['d_test']))))\n",
        "    plt.plot(aux, aux,'--',label = 'y=x')\n",
        "    plt.xlabel('d test')\n",
        "    plt.ylabel('d predicted')\n",
        "    plt.legend()\n",
        "    display(df_aux.round(3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzUmEznmsdYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_pred_vs_test(sample,'A')\n",
        "plt.savefig('generated_figures/zoneA.svg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gt7-yVILsdYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_pred_vs_test(sample,'B')\n",
        "plt.savefig('.\\generated_figures\\zoneB.svg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdEr5l0gsdYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_pred_vs_test(sample,'C')\n",
        "plt.savefig('generated_figures/zoneC.svg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8bP1YpwsdYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_pred_vs_test(sample,'D')\n",
        "plt.savefig('generated_figures/zoneD.svg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjisyoMpsdYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rolling_window = int(N_Episodes/200)\n",
        "for zone in zones:\n",
        "    df_aux=df[df['Zone'] == zone]\n",
        "    average_reward = df_aux['Action 2'].rolling(rolling_window).mean()\n",
        "    plt.plot(np.array(range(len(average_reward)))/len(average_reward), \n",
        "             average_reward, label = 'Box landed in Zone ' + zone)\n",
        "plt.ylim([-0.3,4.5])\n",
        "plt.xlabel('Training progress')\n",
        "plt.ylabel('Action 2 (Avg. over ' + str(rolling_window) + ' episodes)')\n",
        "aux = [0,1]\n",
        "ones = np.ones(2)\n",
        "plt.plot(aux, ones,'b--')\n",
        "plt.plot(aux, 2*ones,'y--')\n",
        "plt.plot(aux, 3*ones,'g--')\n",
        "plt.plot(aux, 4*ones,'r--')\n",
        "plt.plot(aux, np.zeros(2),'k--')\n",
        "# plt.legend(loc='upper left')\n",
        "plt.savefig('generated_figures/action2.svg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA1ymAI8sdYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for zone in zones:\n",
        "    df_aux=df[df['Zone'] == zone]\n",
        "    average_reward = df_aux['Action 1'].rolling(rolling_window).mean()\n",
        "    plt.plot(np.array(range(len(average_reward)))/len(average_reward), \n",
        "             average_reward, label = 'Box landed in Zone ' + zone)\n",
        "plt.ylim([-0.5,4.5])\n",
        "plt.xlabel('Training progress')\n",
        "plt.ylabel('Action 1 (Avg. over ' + str(rolling_window) + ' episodes)')\n",
        "aux = [0,1]\n",
        "ones = np.ones(2)\n",
        "plt.plot(aux, ones,'b--')\n",
        "plt.plot(aux, 2*ones,'y--')\n",
        "plt.plot(aux, 3*ones,'g--')\n",
        "plt.plot(aux, 4*ones,'r--')\n",
        "plt.plot(aux, np.zeros(2),'k--')\n",
        "plt.legend(loc='upper left')\n",
        "plt.savefig('generated_figures/action1.svg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQD9Pk2VsdYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for zone in zones:\n",
        "    df_aux=df[df['Zone'] == zone]\n",
        "    rel_error = abs((df_aux['d_test']-df_aux['d_predicted'])/df_aux['d_test'])\n",
        "    avg_rel_error = rel_error.rolling(rolling_window).mean()\n",
        "    plt.plot(np.array(range(len(avg_rel_error)))/len(avg_rel_error),\n",
        "             avg_rel_error, label ='Zone '+ zone)\n",
        "rel_error = abs((df['d_test']-df['d_predicted'])/df['d_test'])\n",
        "avg_rel_error = rel_error.rolling(5*rolling_window).mean()\n",
        "plt.plot(np.array(range(len(avg_rel_error)))/len(avg_rel_error), \n",
        "         avg_rel_error,  label = 'All zones')\n",
        "plt.xlabel('Training progress')\n",
        "plt.ylabel('Relative error (Avg. over ' + str(rolling_window) + ' episodes)')\n",
        "plt.legend()\n",
        "plt.savefig('generated_figures/Relative-Error.svg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX13WM6EsdYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for zone in zones:\n",
        "    df_aux=df[df['Zone'] == zone ]\n",
        "    rel_error = abs(df_aux['d_test']-df_aux['d_predicted'])\n",
        "    avg_rel_error = rel_error.rolling(rolling_window).mean()\n",
        "    plt.plot(np.array(range(len(avg_rel_error)))/len(avg_rel_error), avg_rel_error, label ='Zone '+ zone)\n",
        "rel_error = abs(df['d_test']-df['d_predicted'])\n",
        "avg_rel_error = rel_error.rolling(5*rolling_window).mean()\n",
        "plt.plot(np.array(range(len(avg_rel_error)))/len(avg_rel_error), avg_rel_error,  label = 'All zones')\n",
        "plt.xlabel('Training progress')\n",
        "plt.ylabel('Absolute error (Avg. over ' + str(rolling_window) + ' episodes)')\n",
        "plt.legend()\n",
        "plt.savefig('generated_figures/Absolute-Error.svg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3ysh_FcsdYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for zone in zones:\n",
        "    dfa=df[df['Zone'] == zone ]\n",
        "    reward = dfa['Reward']\n",
        "    avg_reward = reward.rolling(2*rolling_window).mean()\n",
        "    plt.plot(np.array(range(len(avg_reward)))/len(avg_reward), avg_reward, label ='Zone '+ zone)\n",
        "reward = df['Reward']\n",
        "avg_reward = reward.rolling(10*rolling_window).mean()\n",
        "plt.plot(np.array(range(len(avg_reward)))/len(avg_reward), avg_reward, label = 'All zones')\n",
        "plt.xlabel('Training progress')\n",
        "plt.ylabel('Reward/Episode (Avg. over ' + str(2*rolling_window) + ' episodes)')\n",
        "plt.legend()\n",
        "plt.savefig('generated_figures/rewards.svg')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}